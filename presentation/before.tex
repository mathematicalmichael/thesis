%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation and Terminology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we describe why any of this matters.}
\centering
	Broad Goals of Uncertainty Quantification:
	\vskip 10pt
    \begin{itemize}
	    \item Make inferences and predictions
	    \vskip 20pt
        \item Quantify and reduce uncertainty (aleotoric, epistemic)
	    \vskip 20pt
	    \item Be \emph{accurate} and \emph{precise}
	    \vskip 20pt
	    \item Design ``efficient'' experiments
	    \vskip 20pt
	    \item Collect and use data ``intelligently''
    \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{The one where we define the letters we use and what they mean.}
\begin{itemize}
	\item State variable: $u$ {\color{gray}(e.g. heat, energy, pressure, deflection)}
	\vskip 10pt
	\item<.-> Parameters: $\lambda$ {\color{gray}(e.g. source term, diffusion, boundary data)}
	\vskip 10pt
		\item Deterministic model: $\mathcal{M} (u, \lambda) = 0$, $$\mathcal{M}:\lambda \to u(\lambda)$$

	\item Quantity of Interest map \tdeepred{(QoI)} - at least pcw differentiable \vskip 5pt
		\begin{itemize}
		\item Functional of the solution $$q: u(\lambda) \to \RR$$
		\item Can be vector valued $$Q = \mat{c}{q_1\\ q_2\\ \vdots \\ q_d}$$
		\item<.-> $Q(\lambda) := Q(u(\lambda))$
	\end{itemize}

\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we illustrate how a QoI map relates inputs to outputs.}
%\vskip 25pt
\centering
\begin{figure}
\centering

\begin{tikzpicture}[node distance=2cm, auto,]
 %nodes

% dummy figure to make everything look nice
\node[] (dummy) {};

\node[right=of dummy] (data) {$\dspace = \qoi(\pspace)$};

\node[punkt, inner sep=5pt, below=of dummy] (model) {Model \\ (PDE)}
 	edge[pil,->, bend right=25] node[right] {$Q(u\lam)$} (data);

\node[left=of dummy] (pspace) {$\Lambda$}
   edge[pil,->, bend right=45] node[left] {$u\lam$} (model)
   edge[pil,->, bend left=45] node[above] {$Q\lam$} (data);

\end{tikzpicture}

\vspace{1em}
\emph{Defining the Quantity of Interest Map}
\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Measure Theory 101}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}[t]
% %\vskip 25pt
% \centering
% \begin{figure}
% \centering
%
% The one with the background on inverse problems.
%
% \end{figure}
%
% \end{frame}

%\input{content/intro_slides.tex}
\input{content/problem_formulation.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Philosophical Distinction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we distinguish ourselves from the Bayesian Inverse Problem.}

Bayesian approach: modeling epistemic uncertainties in data on a QoI obtained from a true, but unknown, parameter value, $\paramref$.

\vskip 10pt

\begin{defn}[Deterministic Forward Problem (DFP)]
  Given a space $\pspace$, and QoI map $\qoi$, the \emph{deterministic forward problem} is to determine the values, $\q \in \dspace$ that satisfy
  \begin{equation}
    \q = \qlam, \; \forall \; \param \in \pspace.
  \end{equation}
\end{defn}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we distinguish ourselves from the Bayesian Inverse Problem.}
\begin{defn}[Deterministic Inverse Problem (DIP) Under Uncertainty]
  Given a noisy datum (or data-vector) $d = \q + \xi$, $\q \in \dspace$, the \emph{deterministic inverse problem} is to determine the parameter $\param \in \pspace$ which minimizes
  \begin{equation}
    \norm{\qoi(\param) - d}
  \end{equation}
  where $\xi$ is a random variable (or vector) drawn from a distribution characterizing the uncertainty in observations due to measurement errors.
\end{defn}

In the above definition, $\xi$ is some unobservable perturbation to the true output, arising from epistemic uncertainty (e.g. the precision of available measurement equipment).

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we distinguish ourselves from the Bayesian Inverse Problem.}
The \emph{posterior} is a conditional density, denoted by $\pi_\text{post}(\param\, | \, d)$, proportional to the product of the prior and data-likelihood function \cite{Walpole, Berger, Complete, Smith}:

\begin{equation}\label{eq:sb_post}
    \posterior\lam := \prior\lam \frac{L_\dspace (\q | \param)}{ C },
\end{equation}

\noindent where we emphasize the use of $\posterior$ to distinguish the \emph{posterior} from the updated density $\updated$ in \eqref{eq:updated-pdf}.

\emph{evidence} term $C$ ensures the posterior density integrates to one; given by
\[
C = \int_\pspace \prior\lam L_\dspace(\q | \param) \, d\param.
\]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we provide an illustrative example.}

\begin{itemize}
\item Suppose $\pspace = [-1,1]\subset\RR$ and $Q(\param)=\param^5$ so that $\dspace = [-1,1]$
\item $\initial\sim \mathcal{U}([-1,1])$ and $\observed\sim N(0.25,0.1^2)$
\item $d\in \dspace$ with $d=Q(\paramref)+\xi$ where $\xi\sim N(0,0.1^2)$
\item We then construct $\pi_{\text{post}}(\param \, |\, d)$ for this example assuming a uniform prior (to match the initial density) with an assumed observed value of $d=0.25$ so that the data-likelihood function matches the observed density.
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we provide an illustrative example.}

\begin{figure}
\centering
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-1.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-pf-1.png}
 \caption{(Left) The initial/prior PDF $\initial$ (blue solid curve), updated PDF $\updated$ (black dashed curve), and posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) on $\Lambda$.
 (Right) The push-forward (PF) of the initial/prior PDF $\predicted$ (blue solid curve), observed/likelihood PDF (red solid curve), PF of the updated PDF $\updated$ (black dashed curve), and the PF of the posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) for the QoI.}
 \label{fig:bayes-comparison}
\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we provide an illustrative example.}

\centering
\emph{What happens as we collect more data?}
\vskip 10pt
One approach: \\
SIP: Use $N$ to estimate mean of observed \\
DIP: likelihood function incorporates more data

\begin{figure}
\centering
   \only<1>{
	 \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-5.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-pf-5.png}
	 }
	 \only<2>{
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-10.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-pf-10.png}
	 }
	 \only<3>{
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-20.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-pf-20.png}
	 }
 \label{fig:bayes-comparison-convergence}
\end{figure}

SIP and DIP solutions for varying $N$ for comparison.

\end{frame}
