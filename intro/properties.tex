\subsection{Properties and Assumptions of Consistent Update}\label{sec:properties}
Recall that the SIP is defined as finding a measure $\PP_\pspace$ such that the push-forward of it matched $\observedP$.
The following assumption guarantees the existence of a solution to the SIP in the form of an update to the initial distribution.
It implies that any event which is assigned a positive probability by the observations must also have a positive predicted probability.

\begin{assumption}[Predictability Assumption (Theoretical Form)]\label{as:predicted-theoretical}
  The measure associated with $\observed$ is absolutely continuous with respect to the measure associated with $\observed$.
\end{assumption}

If this is unsatisfied, one source of information (the data) suggests certain events are probable while another source of information (the model and initial beliefs) have a priori ruled that almost surely these events should not occur.
Therefore, either initial beliefs, the model under consideration, or the description of uncertainty encoded in $\predicted$ should be subjected to a critical reevaluation.

The following establishes a more practical form (from the perspective of numerical implementation), of \ref{as:predicted-theoretical} which states that the predicted measure must dominate the observed.
\begin{assumption}[Predictability Assumption (Practical Form)]\label{as:predicted-practical}
The requirement given in Assumption~\ref{as:predicted-theoretical} is guaranteed if the following is satisfied:
\begin{equation}\label{eq:pred-pract}
  \exists \; C>0 \text{ such that } \observed (d) \leq C \predicted(d) \text{ for a.e. } d\in \dspace,
\end{equation}
where it is understood that $\q = \qlam$ for some $\param \in \pspace$.
\end{assumption}

Assumption~\ref{as:predicted-practical} is particularly useful in that it is the same condition required for applying rejection sampling.
Specifically, this allows us to sample from the updated density using the initial density as follows:
(1) draw independent identically distributed (i.i.d.) initial samples $\param_\iparam$, (2) map them through $\qoi$ and (3) evaluate the resulting values through the ratio of observed to predicted densities\footnote{The reason this is possible is that the latter is equivalent to sampling from the observed using samples from the predicted due to the consistency of the updated solution (divide both sides of \eqref{eq:update} by $\initial$ to see the equivalence of ratios).}. The ratio is (4) normalized to have a maximum of one, and (5) the resulting values are compared against random uniform draws, where we (6) keep the samples which corresponded to comparisons where the ratio was the larger value.
Now, assuming \eqref{eq:pred-pract} holds, we state the following theorem from \cite{BJW18} based upon the disintegration of measures:


\begin{thm}[Existence and Uniqueness]
  For any set $A\in \pborel$, the probability measure $\updatedP$ defined by
  \begin{equation}\label{eq:dci_sol}
    \updatedP (A) = \int_\dspace \left (  \int_{\pspace \in \qoi^{-1}(d)}  \initial\lam \frac{\observed\Q}{\predicted\Q} \, d\mu_{\pspace, d} \lam \right ) \, d\dmeas(d), \; \forall \; A \in \pborel
  \end{equation}
  is a consistent solution to the SIP given in (\ref{eq:inverse-problem}), and is uniquely defined up to the specification of the initial probability measure $\initial$ on $(\pspace, \pborel)$.
  Here, $\mu_{\pspace, d}$ denotes the disintegration of the dominating measure $\mu_\pspace$.
\end{thm}

The updated density \eqref{eq:update} in the iterated integral in \eqref{eq:dci_sol} has no normalization constant because it is in fact a density (i.e., it integrates to $1$), which is summarized in Corollary 3.1 in \cite{BJW18} and restated in simplified form below:
\begin{cor}\label{cor:int}
$\updatedP(\pspace) = 1$.
\end{cor}

These definitions are combined to identify the form of the \emph{updated density}, originally derived in \cite{BJW18}:

\begin{defn}[Updated Distribution]\label{defn:updated}
  A solution satisfying \eqref{eq:dci_sol} is referred to as an updated distribution, with an updated density
  \begin{equation}\label{eq:update}
    \updated \lam = \initial \lam \frac{\observed \Q }{\predicted \Q }, \; \forall \; \param \in \pspace.
  \end{equation}
\end{defn}

% Corollary~\ref{cor:int} is critical to understanding some significant differences between the classical Bayesian posterior density~\cite{Smith} and the updated density density given by \eqref{eq:update}, which we discuss in Section~\ref{sec:othermethods} as well as other places throughout this thesis.
% Moreover, this Corollary provides the basis for a useful numerical diagnostic that assesses both the quality of a numerical approximation of $\updated$ based on finite sampling and density estimation as well as any potential violations of the predictability assumption.

% We next turn our attention to attributes of the Data-Consistent framework which make it appealing for use in solving inverse problems.
% Namely, we summarize the stability results first presented in \cite{BJW18} that demonstrate that approximation errors in the updated density are well understood within the measure-theoretic foundation on which the approach is constructed.

%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Stability of the Consistent Solution}\label{sec:stability}
The Total Variation (TV) metric on a space of probability measures, absolutely continuous with respect to a dominating measure $\mu$, is defined as
\begin{equation}\label{eq:tv}
d_{\text{TV}} (\PP_f, \PP_g) := \int \abs{\pp_f - \pp_g} \, d\mu,
\end{equation}
where $\pp_f,\pp_g$ are the densities (Radon-Nikodym derivatives with respect to $\mu$), associated with measures $\PP_f, \PP_g$, respectively.
The stability results below are all with respect to the TV metric, which is widely used in the literature and is also known as \emph{statistical distance}~\cite{GS02, Smith, Silverman}.
We first define stability with respect to perturbations in the data.

\begin{defn}[Stability of Updated Densities I]\label{defn:stableobs}
  Given $\initialP$ and $\observedP$, let $\widehat{\observedP}$ be any perturbation to $\observedP$ on $(\dspace, \dborel)$ satisfying \eqref{eq:pred}.
  Let $\updatedP$ and $\widehat{\updatedP}$ denote the consistent solutions associated with $\observedP$ and $\widehat{\observedP}$, respectively.
  We say that $\updatedP$ is \emph{stable} with respect to perturbations in $\observedP$ if for all $\eps > 0$, there exists a $\delta > 0$ such that
  \begin{equation}
    d_{\text{TV}} (\observedP, \widehat{\observedP}) < \delta \implies d_{\text{TV}} (\updatedP, \widehat{\updatedP}) < \eps.
  \end{equation}
\end{defn}

In \cite{BJW18}, it is shown that $d_{\text{TV}} (\widehat{\updatedP}, \updatedP) = d_{\text{TV}} (\widehat{\observedP}, \observedP)$, which immediately proves the following:

\begin{thm}
  $\updatedP$ is stable with respect to perturbations to $\observedP$.
  \label{thm:stableobs}
\end{thm}

This next definition and result are useful in analyzing the sensitivity of the updated density with respect to the initial beliefs.

\begin{defn}[Stability of Updated Densities II]\label{defn:stableinitial}
  Given $\initialP$ and $\observedP$, let $\widehat{\initialP}$ be any perturbation to $\initialP$ on $(\pspace, \pborel)$ satisfying \eqref{eq:pred}.
  Let $\updatedP$ and $\widehat{\updatedP}$ denote the consistent solutions associated with $\observedP$ and $\widehat{\observedP}$, respectively.
  Let $\sett{\PP_{\pspace, d}}{d\in\dspace}{}$ and $\sett{\widehat{\PP_{\pspace, d}}}{d\in\dspace}{}$ be the conditional probabilities defined by the disintegration of $\initialP$ and $\widehat{\initialP}$, respectively.
  We say that $\updatedP$ is \emph{stable} with respect to perturbations in $\initialP$ if for all $\eps > 0$, there exists a $\delta > 0$ such that for almost every $d\in\supp(\observedP)$,
  \begin{equation}\label{eq:stableinitial}
    d_{\text{TV}} (\PP_{\pspace, d}, \widehat{\PP_{\pspace, d}}) < \delta \implies d_{\text{TV}} (\updatedP, \widehat{\updatedP}) < \eps.
  \end{equation}
\end{defn}

The following important stability theorem is also proven in \cite{BJW18}

\begin{thm}
  $\updatedP$ is stable with respect to perturbations to $\initialP$
  \label{thm:stableinitial}
\end{thm}

Taken together, these stability results provide assurance that the updated density we obtain is accurate up to the level of experimental error polluting $\observedP$ and error in incorrectly specifying initial assumptions using $\initialP$.
Given that specifying the definition of a ``true'' initial density is somewhat nebulous, we are less interested in the consequences of the latter conclusion.
However, generating samples from $\updatedP$ generally requires a numerical approximation to the predicted distribution, which introduces additional errors in $\updatedP$.
In Section~\ref{sec:approx}, the TV metric is used to bound the error in the updated density in terms of the error in the approximation to the push-forward of the initial.




%%%%%%%%% Section 2.3
\subsection{Numerical Approximation and Sampling}\label{sec:approx}
%Since we are given $\initial$ and $\observed$, the computation of $\predicted$ is the only aspect of the Consistent Bayesian framework that needs to be approximated.
%Since there are few restrictions on the structure of the map $\qoi$ that defines $\predicted$, there is in general no explicit expression from which we can generate samples, so we use a numerical approximation to the probability density function.
%
%For simplicity, we simply propagate Monte-Carlo samples from the prior and use a kernel density estimate (usually Gaussian\footnote{In this proposal, all results are generated using this kernel, though six kernels common to density estimation are implemented in the ConsistentBayes Python package [TK - cite Silverman and your github].}).
%
%We summarize this in the following algorithm:
%
%\begin{algorithm}[hbtp]
%\DontPrintSemicolon
%Generate a set of samples $\sett{\param_i}{i=1}{N}$
%	\For{$i = 1, \hdots, N$}{
%			Propagate sample $\param_i$ through the QoI map. Set $d_i = \qoi(\param_i)$.
%	}
%Use $\sett{d_i}{i=1}{N}$ and a density estimation method to approximate $\predicted$.
%	\label{alg:sample}
%\caption{Numerical Approximation of the Push-forward of the Prior Density}
%\end{algorithm}
%


%The computational object associated with $\predicted$ is stored for re-use and can be evaluated at locations in $\dspace$ other than $\sett{d_i}{i=1}{N}$.
%This procedure should be thought of as a characterization of the data space given the prior assumptions encoded in $\initial$.

If $\widehat{\predicted}$ denotes a computational approximation to the push-forward of the initial density obtained with $\widehat{\predicted}$ substituted for $\predicted$ in \eqref{eq:dci_sol}, then the conditional densities from the Disintegration Theorem (c.f. Chapter~\ref{chapter:geometry} for more details), are given as
\[
\frac{\widehat{d\PP_{\pspace, d}}}{d\mu_{\pspace, d}\lam} = \frac{\initial\lam}{ \widehat{\predicted\Q} },
\]
where $\widehat{\PP_{\pspace, d}}$ denotes the disintegration of $\widehat{\updatedP}$.


We assume the following for the approximation of the push-forward of the initial density:
\begin{assumption}\label{as:predicted-theoreticalx}
There exists some $C>0$ such that
\[
\observed (d) \leq C \widehat{\predicted(d)} \text{ for a.e. } d\in \dspace.
\]
\end{assumption}

If this assumption is satisfied, then from \cite{BJW18}, we have the following:
\begin{thm}\label{thm:predicted_bound}
  The error in the approximate updated density is bounded above:
  \begin{equation}\label{eq:predicted_bound}
    d_{\text{TV}} (\updatedP, \widehat{\updatedP}) \leq C d_{\text{TV}} (\predictedP, \widehat{\predictedP}),
  \end{equation}
  where the $C$ is the constant taken from Assumption \ref{as:predicted-theoreticalx}.
\end{thm}

A straightforward approach to construct $\widehat{\predicted}$ is to use a forward propagation of samples from $\pspace$ to $\dspace$ and then apply kernel density estimation (KDE)~\cite{BJW18}.
Then, we may evaluate $\updated$ directly for any sample of $\pspace$ at the cost of one model solve per sample.
While this allows us to incorporate sophisticated sampling techniques such as Markov-Chain Monte-Carlo (MCMC)~\cite{Smith, Tarantola_book} to generate samples according to the updated distribution, we often opt for a simpler route based on rejection sampling by re-using the initial set of propagated samples.
This avoids any additional model evaluations (as would be required by techniques relying on proposal samples such as MCMC).
We leverage the re-use of samples in the results herein extensively.

By Theorem~\ref{thm:predicted_bound}, the accuracy of the computed updated density relies on the accuracy of the approximation of the push-forward of the initial.
Throughout this thesis, we utilize a KDE with a Gaussian kernel to produce the non-parametric estimates of $\predicted$\footnote{
In some examples (particularly those in the appendix), we express $\predicted$ analytically for purposes of demonstration.
Such examples are especially useful for comparing numerical solutions to the exact solutions.
}.
Such KDEs are known to converge at a rate of $\mathcal{O}(N^{-4/(4+\dimD)})$ in mean-squared error and $\mathcal{O}(N^{-2/(4+\dimD)})$ in $L^1$-error, where $\dimD$ is the dimension of $\dspace$, and $N$ is the number of samples from $\initial$ propagated through $\qoi$ [TK - references for convergence rates].

For simplicity, we introduce the following notation to capture the role of the ratio involved in \eqref{eq:dci_sol} to demonstrate properties we can leverage for generating samples from $\updated$.
We let
\[
\updated\lam = \initial \lam r\Q, \text{ where } r\Q = \frac{\observed\Q}{\predicted\Q}
\]

Many standard calculations about the updated density involve integrals of functions of $r\Q$ with respect to the prior.
For any measurable function $f$, we establish the connection of calculating quantities over $\pspace$ with those over $\dspace$ by leveraging the following identity:
\[
\int_\pspace f\left( r\Q \right ) \, d\initialP = \int_\dspace f\left( r(d) \right ) \, d\predictedP
\]

We use several throughout this thesis, including the integral of the updated density:
\[
I(\updated ) = \int_\pspace r\Q \, d\initialP = \int_\dspace r(d) \, d\predictedP ,
\]
which we can use to validate that $I(\updated) \approx 1$ in order to numerically validate that the assumption given in \eqref{eq:predicted_bound} was not violated.

[TK - expand on this]

% Similarly, we follow \cite{BJW18} to write the commonly used metric for Information Gain, the Kullback-Liebler (KL) divergence:
% \begin{equation}\label{eq:KLdiv}
% \text{KL}(\initial : \updated ) = \int_\pspace r\Q \log r\Q \, d\initialP = \text{KL}(\observed : \predicted ),
% \end{equation}
% i.e., the KL-divergence between the initial and updated density is equal to the KL-divergence between the observed and the predicted densities.

\section{Outline of Next Chapters}\label{sec:outline}
Taken together, the results summarized in this section demonstrate that the Data-Consistent framework and density-based solution \eqref{eq:dci_sol} have been rigorously constructed and studied.
They give an experimenter assurance that there are not unexpected consequences for small mistakes in problem formulation.
In the following chapter, we provide a similar sense of assurance for a central practical consideration involved in this work: that the results depend on computational implementations of the aforementioned theory, i.e., software.
We directly address how to establish a complementary level of rigor to the implementation of the work as was invested in the construction of the theory.
We leverage modern advances in software engineering with an emphasis on results being reproducible in an accessible manner.
