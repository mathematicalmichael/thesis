\section{Software Contributions}\label{sec:software-contributions}

As discussed in \ref{sec:motivations}, the entirety of the mathematical content presented has been incorporated into freely available open-source software, including this document.
The novel mathematical developments that have gone into this work are all reflected in various modules and sub-modules as part of the BET Python package.
This software suite follows a number of industry best-practices for code-coverage (\ref{sec:code-coverage}) and continuous integration (\ref{sec:continuous-integration}), i.e. the code is well-tested (\ref{sec:unit-testing}).

A significant effort in the writing of this thesis involved learning about the art and practice of modern (open-source) software development.
The author spent most of 2019 bringing the software in-line with the latest developments in Data-Consistent Inversion.
[TK - later on, put some more in here, we may move this all to a separate chapter, so this section would set the stage for that chapter. i.e. this section is written last.]

%%%%%%%%%%%

\subsection{Software Design and Architecture}\label{sec:architecture}

Having learned a lot about software reproducibility along the way, the author made the decision to treat this thesis as a software project in its own right.
Every example, figure, table, and plot is generated by a combination of Python and Bash scripts contained inside of a public Github repository (www.github.com/mathematicalmichael/thesis).

By hosting this work on Github, corrections can be submitted as Issues, and the document can serve as a reference point for a thorough introduction on the topic of Data-Consistent Inversion.
All the requisite \LaTeX~dependencies are contained in {\tt apt.txt}, and a Jupyterlab environment usable by Binder (which can compile the document and run every example) is configured in the {\tt binder/} directory of the repository (on the binder branch).

Special care is taken to ensure that every file is well-documented.
When appropriate, functions and classes are used in such a way that the same file generates several examples generated.
The parameters required are passed as optional arguments, and Shell scripts containing the exact syntax to generate each figure are included.

For example, to visually demonstrate the implicitly-defined sets of nearest-neighbors in two-dimensional unit domain, we rely on Voronoi-cell diagrams.
One Python file (\bashinline{images/voronoi_unit_domain.py}) contains the methods required to draw a single figure; however, there are many occasions where variation on a plot may be necessary (for example, labels, different line widths).
To accommodate the need for different variations of similar plots, we utilize the argument-parsing package \pythoninline{argparse}, part of the Python standard library, to enable command-line positional and optional arguments \footnote{We equip each function with default values so that the syntax \bashinline{python example.py} without any additional arguments will work, but specific examples rely on optional arguments to be passed accordingly.}.
We include associated (wrapper) files with descriptive names, such as \bashinline{images/make_voronoi_diagrams.sh} that repeatedly call the relevant Python file(s) and passes arguments (such as {\tt num} for ``number of samples'').
We refer the inquisitive reader to  Figure~\ref{fig:voronoi_cells} or \ref{fig:voronoi_issues} to see the Voronoi-cell diagrams generated by the following script\footnote{A \LaTeX macro has been written to allow for Python and Bash files to be displayed contextually to avoid the need to update them in two places. What is shown in the body of this work is an embedded and stylized version what was run to generate figures. When scripts are long, notable sections of code may be interspersed within the text, with the full script available in the appendix associated with the chapter.}:

\bashexternal{images/make_voronoi_diagrams.sh}


These wrappers are called by other Bash scripts which are each responsible for generating figures in different sections or specific models (QoI maps).
This builds something akin to a ``tool chain,'' a series of calls to hierarchies of scripts in order to generate the pictorial content in this work.


%%%%%%%%%%%

\subsubsection{Continuous Integration}\label{sec:continuous-integration}

These outer-level scripts that call others are leveraged by \emph{Travis}, a continuous-integration (CI) service that runs a series of commands on behalf of the user, automating the process of testing functionality.
The familiarity with this technology was introduced as a consequence of working on BET \cite{pyBET}, the software library that originally implemented the set-based approach discussed in \ref{sec:ch02-set}.
The use of CI in BET is similar to that of most software packages written in Python; unit tests are run in some framework (\pythoninline{nose} for BET at first, eventually \pythoninline{pytest}), and a successful run triggers a webhook that communicates with the software repository host (Github) to validate changes.

\subsubsection{Testing}\label{sec:unit-testing}
The aforementioned \emph{unit tests} provide a framework for guaranteeing the behavior of programs or individual methods when instructions are followed.
\emph{Functional tests} encompass more complicated behavior, stringing together several methods/modules to ensure code runs as its author intended, and are usually included when referring to unit testing.
``How to write a test'' is a question that depends highly on the particular functionality, but the premise is always the same.

[TK - maybe show a really basic example of a set/get method and the test for that?]


%%%%%%%%%%%

\subsubsection{Code Coverage}\label{sec:code-coverage}

Code coverage refers to the proportion of lines of code that were run during the process of testing (consisting of unit and functional tests).
The goal is not necessarily to achieve 100\% coverage, but rather to make sure that the most crucial use-cases are checked.
Generally speaking, coverage in the 75-85\% range is industry-standard.

[TK - flesh out a bit more, talk about Codecov, the specific service we use, how it won't be used for the scripts in this thesis].

%%%%%%%%%%%

\subsubsection{BET Architecture}\label{sec:bet-architecture-overview}

The fundamental class in the BET implementation of Data-Consistent Inversion is the \pythoninline{discretization} object, which holds the representation of an inverse problem.

The parameter space $\pspace$ is represented as an \pythoninline{input_sample_set} attribute to the \pythoninline{discretization} instance.
Similarly, $\dspace$ is associated with a \pythoninline{output_sample_set}.
The measure/density $\observedP$ (or $\PP_\dspace$) that is imposed on $\Dspace$ is held as an \pythoninline{output_probability_set} attribute.
Each of these is an instance of a \pythoninline{bet.sample_set} class.

[TK - talk about a few of the core properties of these classes, but perhaps save the diagrams for Ch2].
More details will follow in \ref{sec:ch02-software}, as is an overview of the development timeline.

