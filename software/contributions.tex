\section{Software Contributions}\label{sec:software-contributions}

As discussed in \ref{sec:motivations}, the entirety of the mathematical content presented has been incorporated into freely available open-source software, including this document.
The novel mathematical developments that have gone into this work are all reflected in various modules and sub-modules as part of the BET Python package.
This software suite follows a number of industry best-practices for code-coverage (\ref{sec:code-coverage}) and continuous integration (\ref{sec:continuous-integration}), i.e. the code is well-tested (\ref{sec:unit-testing}).

A significant effort in the writing of this thesis involved learning about the art and practice of modern (open-source) software development.
The author spent most of 2019 bringing the software in-line with the latest developments in Data-Consistent Inversion.


%%%%%%%%%%%

\subsection{Software Design and Architecture}\label{sec:architecture}

Having learned a lot about software reproducibility along the way, the author made the decision to treat this thesis as a software project in its own right.
Every example, figure, table, and plot is generated by a combination of Python and Shell scripts contained inside of a public Github repository (www.github.com/mathematicalmichael/thesis).

By hosting this work on Github, corrections can be submitted as Issues, and the document can serve as a reference point for a thorough introduction on the topic of Data-Consistent Inversion.
All the requisite \LaTeX~dependencies are contained in {\tt apt.txt}, and a Jupyterlab environment usable by Binder (which can compile the document and run every example) is configured in the {\tt binder/} directory of the repository (on the binder branch).

Special care is taken to ensure that every script is at least minimally documented.
When appropriate, functions and classes are used in such a way that the same file generates several examples in this thesis.
The parameters required for the variations are passed as optional arguments, and Shell scripts + makefiles containing the exact syntax to generate each figure are included.
The makefiles are particularly useful in that they track dependencies, so if changes are made to a plotting script, running {\tt make} will re-create only the figures which leverage that changed code.


We illustrate with a specific example: to visually demonstrate nearest-neighbors in two-dimensional unit domain, we rely on Voronoi-cell diagrams.
One Python file (\bashinline{images/voronoi_unit_domain.py}) contains the methods required to draw a single figure; however, there are many occasions where variation on a plot may be necessary (for example, labels, different line widths).
To accommodate the need for different variations of similar plots, we utilize the argument-parsing package \pythoninline{argparse}, part of the Python standard library, to enable command-line positional and optional arguments \footnote{We equip each function with default values so that the syntax \bashinline{python example.py} without any additional arguments will work, but specific examples rely on optional arguments to be passed accordingly.}.
We include associated (wrapper) files with descriptive names, such as \bashinline{images/make_voronoi_diagrams.sh} that repeatedly call the relevant Python file(s) and passes arguments (such as {\tt num} for ``number of samples'').
We refer the inquisitive reader to  Figure~\ref{fig:voronoi_cells} or \ref{fig:voronoi_issues} to see the Voronoi-cell diagrams generated by the following script\footnote{A \LaTeX macro has been written to allow for Python and Bash files to be displayed contextually to avoid the need to update them in two places. What is shown in the body/appendix of this work is an embedded and stylized version of the exact script that was executed to generate figures.}:

\bashexternal{images/make_voronoi_diagrams.sh}


These wrappers are called by other shell scripts which are each responsible for generating figures in different sections or specific models.
This builds something akin to a ``tool chain'' or a ``pipeline,'' a series of calls to hierarchies of scripts in order to generate the pictorial and tabular content in this work.

%%%%%%%%%%%
\subsubsection{Software Dependencies and Docker}\label{sec:docker}
[TK - brief intro to images, containers]
We document the process of how the containers which were used to generate all the results here (the {\tt bin/} directory of this thesis contains the relevant shell scripts), are generated by including the Dockerfiles for them.
This static snapshot allows us to be assured our results can be recreated on x86 architecture (theoretically in perpetuity).
The author also published versions of the relevant images compiled on ARM, and has recreated all figures on a Raspberry Pi as validation.
However, at the time of writing there exists a set of dependencies not yet implemented in an arm-based Dockerfile.
The scientific computing software (largely developed by collaborations among researchers at the national laboratories) FeNiCS, was compiled from source on ARM, as a proof-of-concept.
Generating a docker image which builds the software from source on ARM is a to-do for the author, and can already be theoretically brute-forced by copying over the requisite built-files from the host operating system\footnote{As a stopgap measure for ARM-support, the entire RaspberryPi has been backed up to an image and published online, so that all the dependencies can be resolved by switching microSD cards.}.
FeNiCS is available through conda but only on x86, and no ARM-based docker images are provided to be leveraged.


%%%%%%%%%%%
\subsubsection{Testing}\label{sec:unit-testing}
The aforementioned \emph{unit tests} provide a framework for guaranteeing the behavior of programs or individual methods when instructions are followed.
\emph{Functional tests} encompass more complicated behavior, stringing together several methods/modules to ensure code runs as its author intended, and are usually included when referring to unit testing.
``How to write a test'' is a question that depends highly on the particular functionality, but the premise is always the same.

[TK - maybe show a really basic example of a set/get method and the test for that?]

%%%%%%%%%%%

\subsubsection{Code Coverage}\label{sec:code-coverage}

Code coverage refers to the proportion of lines of code that were run during the process of testing (consisting of unit and functional tests).
The goal is not necessarily to achieve 100\% coverage, but rather to make sure that the most crucial use-cases are checked.
Generally speaking, coverage in the 75-85\% range is industry-standard.

[TK - flesh out a bit more, talk about Codecov, the specific service we use, how it won't be used for the scripts in this thesis].

%%%%%%%%%%%

\subsubsection{Continuous Integration}\label{sec:continuous-integration}

These outer-level scripts that call others are leveraged by continuous-integration (CI) services (Travis, Github Actions), that run a series of commands on behalf of the user, automating the process of testing functionality.
The familiarity with this technology was introduced as a consequence of working on BET \cite{pyBET}, the software library that originally implemented the set-based approach discussed in \ref{sec:ch02-set}.
The use of CI in BET is similar to that of many software packages written in Python; unit tests are run in some framework (\pythoninline{nose} for BET at first, eventually migrated to \pythoninline{pytest} at the author's insistence), and a successful run triggers a webhook that communicates with the software repository host (Github) to validate changes. Github receives messages from the CI service and shows a visual indicator of the status of the work done.

Github Actions came out [TK - date] during the final year of writing this thesis, and provided a convenient way to orchestrate and check on the status of continuous-integration and deployment

\subsubsection{Continuous Deployment to DockerHub and PyPi}
Give an intro to CD principles. Relevant here because the {\tt mud} Python library which implements the analytical expressions presented here and is a shared library for many of the examples, particularly in Chapter~\ref{chapter:mud}.
This library leverages all of the aforementioned principles and components, and additionally automatically publishes to the PyPi registry, allowing anyone to {\tt pip install mud} to resolve \emph{most} of the dependencies of novel contributions presented here.
A similar effort is underway for CD in BET, though the version(s) used here have already been published ({\tt pip install bet}).
Even as changes are made after this thesis is published, readers of this work can rest assured that the results are still reproducible because Github Actions validates all of it on a weekly schedule.
As soon as reproducibility problems surface, the author will be notified so they can be addressed, and the latest build status of the project is visible to the public.
