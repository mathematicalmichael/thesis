\chapter{\uppercase{Impact of Output Quantities on Accuracy} \label{chapter:03}}

Here, we motivate the reduction of a quantity called \emph{skewness} in pursuit of optimizing the geometry of set-valued solutions to stochastic inverse problems with respect to their ability to be well-approximated by Monte-Carlo integration. 
However, the results hold for any attempt to approximate densities defined on sets induced by random samples, and thus may be of interest to the larger research community. 

We demonstrate that the number of samples required to approximate densities using uniform i.i.d.~sampling is proportional to the skewness of the map used for inversion, though the convergence rate of the algorithm used to solve the stochastic inverse problem is unaffected. 


\section{Skewness and Information Content}\label{sec:skewness}
In \cite{BGE+15}, the concept of skewness in a QoI map $\qoi$ was introduced, quantified, and related to the accuracy in solving the SIP with a finite number of samples.
Essentially, skewness is a geometric property that describes how the right angles in generalized rectangles belonging to $\dborel$ are transformed by $\qoi^{-1}$. 
An a priori analysis demonstrated that the number of samples from a {\em regular uniform grid} in $\pspace$ required to approximate the $\PP_\pspace$-measure of $\qoi^{-1}(E)$ to a desired level of accuracy was proportional to the skewness of $\qoi$ raised to the ($d-1$) power where $d$ is the dimension of $\dspace$.
This is a version of the so-called curse-of-dimension.
Since the numerical solution of the SIP relies fundamentally on the approximation of such $\qoi^{-1}(E)$, it was assumed that skewness impacted the probability measure computed using Algorithm~\ref{alg:inv_density} in a similar way.

Skewness was explored further in \cite{BPW17} in the context of optimal experimental design.
There, an additional geometric property of $\qoi$ related to the {\em precision} in the solution of the associated SIP was introduced and quantified. 
Under the same assumption that skewness impacted accuracy of the numerical solution to the SIP, a multi-criteria optimization problem was formulated and solved to determine a $\qoi\in\qspace$ that balanced accuracy and precision in the SIP solution.

Since no previous study has considered the numerical convergence rates of Algorithm~\ref{alg:inv_density}, the impact of skewness on such rates is unclear.
Moreover, it is not clear that skewness causes errors to pollute the entire numerical solution of the SIP since local approximation errors in the $\PP_\pspace$-measure of contour events have a tendency to cancel out over $\pspace$. 
This is the primary focus of the numerical results in Section~\ref{sec:results}. 
Here, for completeness, we define skewness below and refer the interested reader to \cite{BGE+15, BPW17} for more details.


\begin{defn}
For any $\qoi\in\qspace$, $\param \in \pspace$, and a specified row vector $\bf{j}_k$ of the Jacobian $J_{\param, Q}$, we define
\begin{equation}
S_\qoi(J_{\param,Q}, \bf{j}_k) := \frac{\abs{\bf{j}_k} }{\abs{\bf{j}_k^\perp}}.
\label{eq:skewness}
\end{equation}

We define the \textbf{local skewness} of a map $\qoi$ at a point $\param$ as 
\begin{equation}
S_\qoi(\param) = \max_{1\leq k \leq d} S_\qoi(J_{\param,Q}, \bf{j}_k).
\label{eq:localskewness}
\end{equation}
\end{defn}

\begin{defn}
The \textbf{average} \emph{(or \textbf{expected})} \textbf{skewness} is defined as
\begin{equation}
\overline{S_\qoi} = \frac{1}{\mu_{\pspace}(\pspace)} \int_\pspace S_\qoi (\param) \, d\mu_{\pspace}
\label{eq:avgskew}
\end{equation}
\end{defn}

In \cite{BPW17}, it was shown that $S_\qoi(\param)$ can be efficiently computed using a singular value decomposition of the Jacobian $J_{\param,\qoi}$. 
In general, we approximate $\overline{S_\qoi}$ with Monte-Carlo approximations.


%%%% discussion of convergence 

Second, and perhaps most importantly, since the spaces $\pspace$ we are considering are generally bounded and finite, the Hellinger metric metrizes weak convergence (see Thm. 6 in \cite{GS02}).
The latter property is of notable importance because the QoI maps we study are indeed (component-wise) functionals on the space of model inputs $\pspace$. 
Thus, convergence of a sequence of probability measures under the Hellinger metric implies that the QoI's will also converge component-wise in $\RR$. 
In other words, convergence in the Hellinger metric implies the convergence of the sampled QoI map to the exact QoI map since the map is a linear functional of the probability measure. 
In other words, if $\PP_{\pspace,M,N,h}$ converges to either $\PP_{\pspace,M,N}$, $\PP_{\pspace,M}$, or $P_\pspace$ using the Hellinger metric, this implies that the error converged to zero in the numerically computed $Q(\param^{(j)})$.
Thus, convergence in the Hellinger metric implies, in a sense, convergence of the numerical method used to construct the QoI map. 
Furthermore, recall that weak convergence $\PP_n \to \PP$ is defined to mean 
\[
\int f \PP_n \to \int f \PP
\]
for bounded Lipschitz functions $f:\pspace\to\RR$. 
Taking $f = \Chi_A$, this leads to the following implication:  
\[
\PP_{\pspace, M, N} \to P_\pspace \implies \PP_{\pspace, M, N} (A) \to P_\pspace (A) \quad \forall{A\in\pborel}.
\]
%provided we rigorously define $\P\PP_{\pspace, M, N}$ to measure sets in $\BB_\pspace$, which we proceed to do in the following section\footnote{\bf{I know this is clumsy, but I'm not exactly sure how to phrase this correctly, because it almost seems like this would be true for all $A\in\pspace$ instead. I suppose the omission of the differential operator above is intentionally vague to gloss over this detail. Can we sharpen this up?}}
It is a combination of computational ease of implementation and theoretical implications that motivates the choice of the Hellinger distance as the metric used in the numerical results of Section \ref{sec:results}.



\
\section{Accuracy of Set-Based Inversion}\label{sec:ch03-set}

%%%%%%%%%%%%%% discretization discussion, software contribution in later section



The measures computed from Algorithm~\ref{alg:inv_density} are defined on a set of samples $S = \set{\param^{(j)}}_{j=1}^{N}$ which implicitly define a Voronoi-cell partition $\set{\VV^{(j)}}_{j=1}^{N}$ of the parameter space $\pspace$. 
We let $\BB_{\pspace, N}$ denote the \emph{computational algebra} generated by $\set{\VV^{(j)}}_{j=1}^{N}$, i.e., using standard measure theory notation, 
$$
	\BB_{\pspace,N} = \sigma\left(\set{\VV^{(j)}}_{j=1}^{N}\right).
$$
Clearly, $\BB_{\pspace, N}\subset \BB_\pspace$ and the events $A\in B_{\pspace, N}$ represent the $A\in\BB_\pspace$ for which we can ``easily'' compute probabilities and make inferences.
While Algorithm~\ref{alg:inv_density} ultimately defines a probability measure implicitly on $(\pspace,\BB_\pspace)$, computationally this is almost never done and the measures are only interrogated on the computational algebra associated with the set of samples. 

Different sets $S_k = \set{\param{(i)}}_{i=1}^{N_k}$, where the $\param^{(i)}$'s and $N_k$'s may be completely different for each $k$, will lead to different measures computed from Algorithm~\ref{alg:inv_density}. 
Each $S_k$ induces a computational algebra which we index using the notation $\BB_{k}$ for simplicity, where it is understood that $\BB_k = \BB_{\pspace, N_k}$. 

This poses an immediate problem with respect to a computational approach to computing $d_H$: how do we compare measures $\PP_{\pspace, M, N_1}$ and $\PP_{\pspace, M, N_2}$ which may be defined on completely different computational algebras (even if $N_1=N_2$)?
See Figure~\ref{fig:voronoi_issues} for an illustration of such a scenario.

\begin{figure}[ht]
\centering
	\begin{minipage}{.4875\textwidth}
		\includegraphics[width=\linewidth]{./images/voronoi_diagram_N25_r0}
	\end{minipage}
		\begin{minipage}{.4875\textwidth}
		\includegraphics[width=\linewidth]{./images/voronoi_diagram_N25_r10}
	\end{minipage}
\caption{
Two different Voronoi partitions induced by $\nsamps_1 = \nsamps_2 = 25 $ uniform i.i.d.~random samples. 
}
\label{fig:voronoi_issues}
\end{figure}


The proof of the following Lemma describes how to ``computationally extend'' {\em any} probability measure defined on a computational algebra to a full $\sa$ $\BB$, which we exploit in Algorithm~\ref{alg:hellinger_disc}.
\begin{lem}
\label{lem:measuresets}
Let $\mu$ be a measure on $(\pspace, \BB_\pspace)$, $\set{\VV^{(j)}}_{j=1}^{N}$ be a partition of $\pspace$, and $\BB_{\pspace, N}$ the computational algebra generated by $\set{\VV^{(j)}}_{j=1}^{N}$. 
Assume $\mu (\VV^{(j)}) > 0 \; \forall \; j=1,\hdots, N$. 
Then, there exists a probability measure $\eta$ on $(\pspace, \BB_\pspace)$ such that $\eta(A) = \eta_N(A) \; \forall \; A\in\BB_{\pspace, N}$. 
\end{lem}
In the proof below, we use $\eta_N$ and $\mu$ to construct a type of ``discrete'' Radon-Nikodym derivative of $\eta$. 
This is motivated by the formal structure of solutions given by Algorithm~\ref{alg:inv_density}.
The proof of Lemma~\ref{lem:measuresets} can be found in Appendix~\ref{app:measuresets}, but the two key equations involved are reproduced here for later reference:

\begin{equation}\label{eq:finiteradon}
f_N (\param) = \sum_{j=1}^{N} \frac{\eta_N (\VV^{(j)}) }{\mu (\VV^{(j)})} \Chi_{\VV^{(j)}} (\param).
\end{equation}

Then, for any $A\in\BB_\pspace$, define

\begin{equation}\label{eq:approxmeasure}
\eta (A) = \int_A f_N (\param) \, d\mu.
\end{equation}

We note that in practice, $\Chi_{\VV^{(j)}} (\param)$ requires the use of nearest-neighbor computations, but otherwise evaluation of Eq.~\eqref{eq:finiteradon} is straightforward to compute.
With that established, we now present the algorithm used for approximating the distances between pairs of measures (with the computational implementation discussed in \ref{sec:ch03-software}.


\begin{algorithm}
\DontPrintSemicolon
\caption{Hellinger Discretization}
\label{alg:hellinger_disc}
Let $(\pspace, B_{\pspace, N_1}, \eta_{N_1} )$ and $(\pspace, B_{\pspace, N_2}, \eta_{N_2} )$ be given.\\
 
Construct $f_{N_1}$ and $f_{N_2}$ and corresponding $\eta_1, \eta_2$ using Eq.~\eqref{eq:finiteradon} and Eq.~\eqref{eq:approxmeasure}, respectively.

Use Monte Carlo sampling to approximate
$$ d^2_H(\eta_1, \eta_2) = \int_\pspace \sqrt{f_{N_1}(\param}) - \sqrt{f_{N_2}(\param)} \, d\mu $$.
\end{algorithm}

Since we now have a way to extend probability measures defined on $(\pspace, \BB_{\pspace, N})$ to  probability measure on $(\pspace, \BB_{\pspace})$, we can use simple Monte-Carlo approximation schemes to the Hellinger distance between two probability measures defined on two separate computational algebras. 
This is demonstrated in Algorithm~\ref{alg:hellinger_disc}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{experimental_setup.tex}

\input{rotation_example.tex}
\input{skew_example.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\
\section{Accuracy of Sample-Based Inversion}\label{sec:ch03-sample}

How does the new approach compare? What role does the KDE play in the error?
Focus on linear problems. One or two examples (perhaps use that skew-map with 1 and 2 and 4).

%%%% Sample-Based Skewness Example %%%%%% 

\subsection{A Sample-Based Solution}\label{eq:sampleskew}

Here we solve the problems in Examples~\ref{ex:skewness} and~\ref{ex:3dmap}, except in the framework of the Sample-Based Inversion outlined in \ref{sec:ch02-sample}.
Effect of Skewness appears to be mitigated.
\vfill{10in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\
\section{Software Contributions}\label{sec:ch03-software}

Talk about the BET module that computes metrics.
Discuss testing, show sample of usage (disconnected from skewness, high-level).

\
\section{Numerical Results and Analysis}\label{sec:ch03-approx}

\subsection{Convergence with Repeated Observations}
2D but with repeated data, show how the measures converge. Should be identical to the differences in the observed densities. 

\subsection{Nonlinear Example}
Maybe heatrod here?

%%%%%%%%%%%%%%

Pick an ODE problem and show how the results look like (assimilating as many data points as inputs, if using time-series model). 

Our goal in this section is to provide a set of examples that demonstrate these two approaches, their ``solutions.''
Exponential Decay, uncertain initial condition and rate. Fix two measurement times. No OED discussion.

Show visualization of solution on voronoi-mesh vs. 2D density. 

This is going to set up the stage nicely.
What if we had more measurements to incorporate? Discuss how the distributions we imposed as our observed were kind of a little disingenuous since they were based on single measurements. Well, rather, they represent the answer to a different question.

