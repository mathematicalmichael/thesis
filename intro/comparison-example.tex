\subsection{Illustrative Example}
Despite the differences in the statistical interpretations, we formulate a problem where $\observed$ and $\initial$ match the forms of $L_\dspace$ and $\prior$.
However, we still observe differences between $\updated$ and $\posterior$ due to the use of a normalizing constant $C$ in $\posterior$ and the use of $\predicted$ in $\updated$.
We explore the impact of this difference in the denominators of the solutions in the following example.

%%%%%


\begin{ex}
Suppose $\pspace = [-1,1]\subset\RR$ and $Q(\param)=\param^5$ so that $\dspace = [-1,1]$.
For the data-consistent framework, we assume $\initial\sim \mathcal{U}([-1,1])$ and $\observed\sim N(0.25,0.1^2)$.
The push-forward of initial PDF, the observed PDF, and the updated PDF are shown in Fig.~\ref{fig:bayes-comparison}.

For the Bayesian inverse problem, we assume $d\in \dspace$ with $d=Q(\paramref)+\xi$ where $\xi\sim N(0,0.1^2)$.
%In particular, we assume that $d=0.25$ and follow the process of \cite{Stuart10} to form the data-likelihood function so that it matches the observed density.
We then construct $\pi_{\text{post}}(\param \, |\, d)$ for this example assuming a uniform prior (to match the initial density) with an assumed observed value of $d=0.25$ so that the data-likelihood function matches the observed density.
The posterior and its push-forward are also shown in Fig.~\ref{fig:bayes-comparison}.


\begin{figure}[htbp]
\centering
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-1.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-pf-1.png}
 \caption{(Left) The initial/prior PDF $\initial$ (blue solid curve), updated PDF $\updated$ (black dashed curve), and posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) on $\Lambda$.
 (Right) The push-forward (PF) of the initial/prior PDF $\predicted$ (blue solid curve), observed/likelihood PDF (red solid curve), PF of the updated PDF $\updated$ (black dashed curve), and the PF of the posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) for the QoI.}
 \label{fig:bayes-comparison}
\end{figure}


While the updated and posterior densities in Fig.~\ref{fig:bayes-comparison} share certain similarities (e.g., they are uni-modal with similar locations of the mode), they are otherwise visibly distinct.
The differences between these densities is made more evident by examining their push-forwards.
The push-forward of the updated density agrees well with the observed density, which is to be expected.
However, the push-forward of the posterior is bi-modal and does not match the observed density, which we recall is identical to the data-likelihood function in this case.
%with peaks that appear to align fairly well with the two distinct peaks of the predicted density and observed density.
%Recall that the observed density and data-likelihood are, in this case, identical.
%Moreover, with the setup described above, the predicted density can also be interpreted as the push-forward of the prior density.
%This demonstrates the regularizing impact of the prior on the posterior and how i.

%
%Hierarchical Bayesian methods \cite{} extend this typical framework to problems where aleatoric uncertainties are present, but are still fundamentally developed from a  point estimation perspective.
%Specifically, prior distributions are specified from a parametric family of distributions, such as Gaussian distributions, and the hyper-parameters used to define that family of distributions, such as the means and variances, become a focal point of estimation by the methodology.

\end{ex}

The takeaway to the above discussion and example is that each density is solving a {\em different} inverse problem.
The posterior density is intended to provide point estimates of a true parameter value whereas the updated density is intended to quantitatively characterize natural variations in parameter values.
We reformulate the previous example to make the role of data collection more central in the follow example.

\begin{ex}
For the (Bayesian) Deterministic Inverse Problem, suppose $Q(\paramref)=0.25$ and noisy measurement data are drawn from a $N(0.25,0.1^2)$, i.e., we assume that each datum is given by $d=Q(\paramref)+\xi$ where $\xi\sim N(0,0.1^2)$.
For the SIP, we use the sample mean and variance of data to estimate the ``exact'' observed $N(0.25,0.1^2)$ distribution.
The observed density and data-likelihood become significantly different from one another as more data are collected.
The data-likelihood is in fact given by a product of normal densities.

We draw $M=5, 10, \text{ and}, 20$ samples to form estimates of $\observed$ and the likelihood functions.
We show the results in Figure~\ref{fig:bayes-comparison-convergence}.

\begin{figure}[htbp]
\centering
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-5.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-pf-5.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-10.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-pf-10.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-20.png}
   \includegraphics[width=0.49\linewidth]{figures/bip-vs-sip-pf-20.png}
 \caption{(Top to Bottom): $S=5, 10, \text{ and}, 20$ samples are used to solve the SIP and DIP for comparison. (Left) The initial/prior PDF $\initial$ (blue solid curve), updated PDF $\updated$ (black dashed curve), and posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) on $\Lambda$.
 (Right) The push-forward (PF) of the initial/prior PDF $\predicted$ (blue solid curve), observed/likelihood PDF (red solid curve), PF of the updated PDF $\updated$ (black dashed curve), and the PF of the posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) for the QoI.}
 \label{fig:bayes-comparison-convergence}
\end{figure}

For all values of $M$, the push-forward of the initial remains the same, and the push-forward of the update matches the observed.
By contrast, the posterior increases in confidence alongside the predictions it produces.
This further illustrates that the DIP and SIP are fundamentally different problems (they are addressing different questions).
As more data are incorporated, the goal of the DIP is to reduce epistemic uncertainty; for the SIP, it is to quantify the aleotoric uncertainty.

\end{ex}


%%%%%

In summary, it is not the goal of Bayesian inference to construct a pullback distribution.
Bayesian inverse problems are fundamentally posed as parameter-identification problems, not distribution estimation problems.
However, one could assume that a posterior on $\pspace$ can be expressed as a Gaussian distribution, and solve for the most likely mean and standard deviation that characterizes it \citep{Smith, Tarantola_book}.
This defines what is commonly referred to a as a Hierarchical Bayesian Inverse Problem.

More complex densities can be approximated by mixture models.
For example, one can assume that the posterior can be given by a linear combination of four Gaussian distributions, and solve for eight parameter values (four standard deviations and means).
However, the operative word here is \emph{assume}; in order to capture a density using a Bayesian framework, one needs to impose some sort of explicit structure on the posterior.
No such assumption is required in the DCI framework.
Distributions (or measures) can be solved for directly, regardless of any nonlinear/non-parameteric structure by leveraging the measure-theoretic approaches described in \cite{BE13} or \cite{BJW18a}.

It is important to note that Hierarchical Bayesian inverse problem still casts a distribution-estimation problem in the context of parameter identification.
As a complementary line of reasoning, we seek to formulate a parameter identification problem in a DCI framework.
For example, the mean of the updated density could be used as an estimator to address the parameter identification problem.
However, collecting more data does not improve confidence if used to estimate $\observed$ as described above unless we revisit the results with a focus on parameter estimation by considering alternative data-driven ways to construct the QoI (and subsequently $\observed$).

In Chapter~\ref{chapter:mud}, we motivate the use of the maximal updated density point (maximizing the update), as a means of providing a useful point estimate to parameters.
Before we proceed, we finish summarizing key results about the stability and numerical convergence of the updated solution \eqref{eq:updated-pdf} in the next section.

\FloatBarrier
