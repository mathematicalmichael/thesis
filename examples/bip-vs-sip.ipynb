{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bayesian Inverse Problem (BIP) vs Stochastic Inverse Problem (SIP)\n",
    "---\n",
    "\n",
    "This notebook explores the differences between the BIP and SIP and their respective solutions.\n",
    "\n",
    "While we consider the example from Section 7 in [Combining Push-Forward Measures and Bayes' Rule to Construct Consistent Solutions to Stochastic Inverse Problems](https://epubs.siam.org/doi/abs/10.1137/16M1087229), we use the more recent notation/terminology first introduced in [Convergence of Probability Densities Using Approximate Models for Forward and Inverse Problems in Uncertainty Quantification](https://epubs.siam.org/doi/abs/10.1137/18M1181675).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIP and SIP setups, solutions, and interpretations.\n",
    "---\n",
    "\n",
    "For the BIP, we assume that observational data $q\\in\\mathcal{D}$ are given in the form of\n",
    "$$\n",
    "    q=Q(\\lambda)+\\eta\n",
    "$$\n",
    "where $\\eta\\sim N(0,\\sigma^2)$ and for simplicity we assume the standard deviation $\\sigma$ is known.\n",
    "The use of an additive unbiased noise model following a Gaussian distribution with known variance is common in the formulation of a BIP.\n",
    "If the noise model is not known exactly, then forms of it are often proposed and priors may be placed on the non-physical parameters such as $\\sigma$ (called hyper-parameters), and these hyper-parameters become part of the parameter estimation problem.\n",
    "This just introduces an additional layer of assumptions and computational complexity that we do not concern ourselves with here. \n",
    "\n",
    "The objective of the BIP is to determine the physical parameter $\\lambda$ that can explain all of the observed data. \n",
    "To this end, a data-likelihood function is formed, which usually takes the form of a conditional density (or is proportional to if normalizing constants are ignored, which is often the case with the BIP)\n",
    "$$\n",
    "    L(q\\, | \\, \\lambda) = \\rho(q-Q(\\lambda))\n",
    "$$\n",
    "for a single datum $q$ where $\\rho$ is the density associated with the $N(0,\\sigma^2)$ distribution.\n",
    "If more data are observed, they are conceptually organized into a vector $q\\in\\mathbb{R}^m$ (where $m$ denotes the number of i.i.d. observed data points) and the data-likelihood function takes the form\n",
    "$$\n",
    "    L(q\\, | \\, \\lambda) = \\Pi_{i=1}^m \\rho(q_i - Q(\\lambda)).\n",
    "$$\n",
    "The objective of the BIP is evident in the form of such data-likelihoods. First of all, the statistical interpretation of a conditional density of the form (or proportional to) $L(q\\, | \\, \\lambda)$ is to assess the relative likelihoods that different values of $\\lambda$ could have produced the data vector $q$. Second of all, as more data are collected (through assumed repeated i.i.d. experiments where a true $\\lambda$ value is assumed fixed across all such experiments), then $L(q\\, | \\, \\lambda)$ will become more \"peaked\" around $Q(\\lambda_{true})$ by design (we will see the impact of this concentration of probability in the example below).\n",
    "\n",
    "To construct the posterior density on parameter space that \"solves\" the BIP and is used to assess the relative likelihoods of any particular parameter being the one true fixed parameter given the observed data, a prior description of uncertainty in any particular parameter being the \"one true\" parameter is required.\n",
    "Let $\\pi^{prior}_\\Lambda(\\lambda)$ denote this prior. \n",
    "Then, the posterior is a conditional density, denoted by $\\pi^{post}_\\Lambda(\\lambda\\, | \\, q)$ that is proportional to the prior and data-likelihood functions.\n",
    "That is to say, \n",
    "$$\n",
    "    \\pi^{post}_\\Lambda(\\lambda \\, | \\, q) \\propto \\pi^{prior}_\\Lambda(\\lambda) L(q\\, | \\, \\lambda).\n",
    "$$\n",
    "\n",
    "Here is a good question: why on earth did Troy put \"solves\" in quotes? Because the posterior is just a means to an end. Most people actually treat parameter estimates related to the posterior to be the actual solutions to the BIP since that is what they are after. The posterior provides a way to describe the uncertainty in a point estimate.\n",
    "\n",
    "At this point, you may find reading the introduction of [Data-consistent inversion for stochastic input-to-output maps](https://iopscience.iop.org/article/10.1088/1361-6420/ab8f83) a useful activity to help understand the differences in perspectives on the quantitative characterizations of uncertainty between the BIP and SIP. In particular, there is a good deal of discussion about the point of the BIP, the posterior, and typical point estimates (citations are also provided). \n",
    "\n",
    "Moving on to the SIP, the objective is to determine a distribution on the physical parameters $\\lambda$ that can explain the variation in the observed data (i.e., the induced distribution on the data space defined by a push-forward is consistent with what is observed). \n",
    "The details of this are described in [Combining Push-Forward Measures and Bayes' Rule to Construct Consistent Solutions to Stochastic Inverse Problems](https://epubs.siam.org/doi/abs/10.1137/16M1087229) and later summarized in both [Convergence of Probability Densities Using Approximate Models for Forward and Inverse Problems in Uncertainty Quantification](https://epubs.siam.org/doi/abs/10.1137/18M1181675) and [Data-consistent inversion for stochastic input-to-output maps](https://iopscience.iop.org/article/10.1088/1361-6420/ab8f83). \n",
    "\n",
    "Using the notation/terminology of the later work, we first put an initial density, $\\pi^{init}_\\Lambda(\\lambda)$ to describe some initial distribution on parameter space that *most assuredly is **not** a consistent distribution.* This is simply meant to be applied on the generalized contour structure induced by $Q^{-1}$ so that we have some means of defining how probabilities should be distributed in directions not informed by the observable quantities.\n",
    "In the absence of any \"good idea\" about what this should be, we often apply a uniform distribution to represent all the possible variations of physical parameters being equally likely \"as far as we know.\" \n",
    "After all, unless there is evidence/knowledge to the contrary, why should we impose anything other than this?\n",
    "\n",
    "The push-forward of this initial density through the map $Q$ produces a *predicted* density, $\\pi^{pred}_\\mathcal{D}(q)$, which describes the relative likelihoods of various data we may observe before we have any actual data to suggest otherwise. This can be evaluated at any $q\\in \\mathcal{D}=Q(\\Lambda)$, but as we will see, it is the evaluation of this density not at arbitrary data $q$ but at *predicted* data $Q(\\lambda)$ associated with some proposed parameter $\\lambda$ that makes all the difference.\n",
    "\n",
    "Once an observed density is given/prescribed/estimated on $\\mathcal{D}$, which we denote by $\\pi^{obs}_\\mathcal{D}(q)$, we can then formulate the ratio\n",
    "$$\n",
    "    r(\\lambda) = \\frac{\\pi^{obs}(Q(\\lambda))}{\\pi^{pred}(Q(\\lambda))},\n",
    "$$\n",
    "(which is sometimes denoted by $r(Q(\\lambda))$ just to emphasize the dependence on the map $Q$), which serves as a multiplicative update to the initial density.\n",
    "In other words, we *update* the initial density to create a new density\n",
    "$$\n",
    "    \\pi^{update}_\\Lambda(\\lambda) = \\pi^{init}_\\Lambda(\\lambda)r(\\lambda).\n",
    "$$\n",
    "\n",
    "This form of the updated density follows form the Disintegration Theorem where the disintegration of the initial density is used to create the family of conditional densities on the contours defined by $Q^{-1}(q)$ for a.e. $q\\in\\mathcal{D}$. \n",
    "That this is an actual density requires a **predictability assumption**.\n",
    "Moreover, the predictability assumption coupled with the Disintegration Theorem then guarantees that $\\pi^{update}$ is not simply *an* update to the initial density but is *also* a consistent solution to the SIP meaning that its push-forward matches $\\pi^{obs}_\\mathcal{D}(q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toy example to compare the BIP and SIP\n",
    "---\n",
    "\n",
    "$Q(\\lambda)=\\lambda^5$, $\\Lambda=[-1,1]$, and the initial and prior densities are chosen to both be uniform on $\\Lambda$. \n",
    "\n",
    "The data, $q$, follow a $N(\\mu,\\sigma^2)$ distribution.\n",
    "The question is: do we formulate a BIP where we assume that there is a true parameter $\\lambda^\\star$ such that $\\mu=Q(\\lambda^\\star)$ so that any individual datum $q=Q(\\lambda^\\star)+\\eta$ where $\\eta\\sim N(0,\\sigma^2)$ with known $\\sigma$, or do we simply estimate $\\pi^{obs}(q)$ from the data and solve the SIP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# The libraries we will use for part 1 of talk\n",
    "import numpy as np\n",
    "from scipy.stats import norm, uniform # The standard Normal distribution\n",
    "from scipy.stats import gaussian_kde as kde # A standard kernel density estimator\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "if presentation:\n",
    "    matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "    matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['font.size'] = 32\n",
    "matplotlib.rcParams['figure.figsize'] = 10,5\n",
    "matplotlib.backend = 'Agg' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def QoI(lam,p): # defing a QoI mapping function as monomials to some power p\n",
    "    q = lam**p\n",
    "    return q\n",
    "\n",
    "def data_likelihood(qvals, data, num_data):\n",
    "    v = 1.0\n",
    "    for i in range(num_data):\n",
    "        v *= norm.pdf(qvals-data[i], loc=0, scale=sigma)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "N, mu, sigma = int(1E4), 0.25, 0.1 # number of samples from initial and observed mean (mu) and st. dev (sigma)\n",
    "lam = np.random.uniform(low=-1,high=1,size=N) # sample set of the initial\n",
    "\n",
    "# Evaluate the QoI map on this initial sample set to form a predicted data set\n",
    "qvals_predict = QoI(lam,5) # Evaluate lam^5 samples\n",
    "\n",
    "# Estimate the push-forward density for the QoI\n",
    "pi_predict = kde( qvals_predict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Something somewhat silly/unrealistic but it sets the stage.\n",
    "---\n",
    "\n",
    "Suppose we just \"know\" that $\\mu=0.25$ and we use this to formulate the SIP and BIP  where we just assume we observed the mean as the datum for the BIP.\n",
    "\n",
    "This will never happen, but it is illustrative of the differences between the BIP and SIP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, now let's simulate data and do something more realistic\n",
    "---\n",
    "\n",
    "Now we simulate data. You can change the `num_data` to see what would happen as data are increased. Notice that the BIP solution and push-forward are very stable with low sample size.\n",
    "\n",
    "Also, notice that even if we estimate a \"true parameter\" with the BIP, e.g., using the mean of the updated density, then this is also generally closer to what the parameter value is that corresponds to $\\mu\\in\\mathcal{D}$ than the MAP point (the maximum posterior point). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute more observations for use in BIP\n",
    "for num_data in [1, 5, 10, 20]:\n",
    "    np.random.seed(123456) # Just for reproducibility, you can comment out if you want.\n",
    "\n",
    "    data = norm.rvs(loc=mu, scale=sigma**2, size=num_data)\n",
    "\n",
    "    # We will estimate the observed distribution using a parametric estimate to keep\n",
    "    # the assumptions involved as similar as possible between the BIP and the SIP\n",
    "    # So, we will assume the sigma is known but that the mean mu is unknown and estimated\n",
    "    # from data to fit a Gaussian distribution\n",
    "    mu_est = np.mean(data) \n",
    "\n",
    "    r_approx = np.divide(norm.pdf(qvals_predict, loc=mu_est, scale=sigma), pi_predict(qvals_predict))\n",
    "\n",
    "    # Use r to compute weighted KDE approximating the updated density\n",
    "    update_kde = kde( lam, weights=r_approx )\n",
    "\n",
    "    # Construct estimated push-forward of this updated density\n",
    "    pf_update_kde = kde( qvals_predict, weights=r_approx)\n",
    "\n",
    "    likelihood_vals = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        likelihood_vals[i] = data_likelihood(qvals_predict[i], data, num_data)\n",
    "\n",
    "    # compute normalizing constants\n",
    "    C_nonlinear = np.mean(likelihood_vals)\n",
    "    data_like_normalized = likelihood_vals/C_nonlinear\n",
    "\n",
    "    posterior_kde = kde( lam, weights=data_like_normalized )\n",
    "\n",
    "    # Construct push-forward of statistical Bayesian posterior\n",
    "    pf_posterior_kde = kde( qvals_predict, weights=data_like_normalized )\n",
    "\n",
    "    fig = plt.figure() # Plot the initial and posterior\n",
    "    lam_plot = np.linspace(-1,1, num=1000)\n",
    "    plt.plot(lam_plot, uniform.pdf(lam_plot, loc=-1, scale=2), 'b--', linewidth=4, label=\"Initial/Prior\")\n",
    "    plt.plot(lam_plot, update_kde(lam_plot),'k-.', linewidth=4, label=\"Update\")\n",
    "    post_plot = plt.plot(lam_plot, posterior_kde(lam_plot), 'g:', linewidth=4, label=f'Posterior, $N={num_data}$')\n",
    "\n",
    "    plt.xlim([-1,1]), plt.ylim([0,24]);\n",
    "    plt.xticks(fontsize=24), plt.yticks(fontsize=24);\n",
    "    plt.xlabel(\"$\\Lambda$\",fontsize=24)\n",
    "    plt.legend(fontsize=24, loc='upper left')\n",
    "    plt.savefig(f'bip-vs-sip-{num_data}.png', bbox_inches='tight'), plt.show();\n",
    "\n",
    "\n",
    "    plt.figure() # Plot the push-forward of the initial, observed density, and push-forward of pullback and stats posterior\n",
    "    qplot = np.linspace(-1,1, num=1000)\n",
    "    plt.plot(qplot,norm.pdf(qplot, loc=mu, scale=sigma), 'r-', linewidth=4, label=\"$N(0.25,0.1^2)$\")\n",
    "    plt.plot(qplot, pi_predict(qplot),'b-.', linewidth=4, label=\"PF of Initial\")\n",
    "    plt.plot(qplot, pf_update_kde(qplot),'k--', linewidth=4, label=\"PF of Update\")\n",
    "    plt.plot(qplot, pf_posterior_kde(qplot),'g:', linewidth=4, label=\"PF of Posterior\")\n",
    "\n",
    "    plt.xlim([-1,1]), plt.ylim([0,24]);\n",
    "    plt.xticks(fontsize=24), plt.yticks(fontsize=24);\n",
    "    plt.xlabel(\"$\\mathcal{D}$\", fontsize=24)\n",
    "    plt.legend(fontsize=24, loc='upper left')\n",
    "    plt.savefig(f'bip-vs-sip-pf-{num_data}.png', bbox_inches='tight'), plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if presentation:\n",
    "    !mv bip-vs-sip*.png ../presentation/figures/\n",
    "else:\n",
    "    !mv bip-vs-sip*.png ../figures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "482.913px",
    "left": "126px",
    "top": "174.923px",
    "width": "494px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
