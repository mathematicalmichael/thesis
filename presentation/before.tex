%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation and Terminology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we describe why any of this matters.}

{\bf Broad Goals of Uncertainty Quantification}

\bigskip
    \begin{itemize}
	    \item Make inferences and predictions

			\bigskip
        \item Quantify and reduce uncertainties (aleotoric, epistemic)

			\bigskip
	    \item Be \emph{accurate} and \emph{precise}

			\bigskip
	    \item Design ``efficient'' experiments

			\bigskip
	    \item Collect and use data ``intelligently''
    \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we define the letters we use and what they mean.}
\begin{itemize}
	\item State variable: $u$ {\color{gray}(e.g. heat, energy, pressure, deflection)}

	\bigskip
	\item<.-> Parameters: $\lambda$ {\color{gray}(e.g. source term, diffusion, boundary data)}

	\bigskip
		\item Model: $\mathcal{M} (u, \lambda) = 0$, so $u(\param)$

	\bigskip
	\item Quantity of Interest \tdeepred{(QoI)} map, (piecewise smooth):

		$$Q = \mat{c}{q_1\\ q_2\\ \vdots \\ q_\dimD}, \; \text{ where } \; q_i: u(\lambda) \to \RR$$

	\bigskip
	\item We write $Q(\lambda) := Q(u(\lambda))$ to make the dependence on $\param$ explicit.
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we illustrate how a QoI map relates inputs to outputs.}
%\vskip 25pt
\centering
\begin{figure}
\centering

\begin{tikzpicture}[node distance=2cm, auto,]
 %nodes

% dummy figure to make everything look nice
\node[] (dummy) {};

\node[right=of dummy] (data) {$\dspace = \qoi(\pspace)$};

\node[punkt, inner sep=5pt, below=of dummy] (model) {Model \\ (e.g., PDE)}
 	edge[pil,->, bend right=25] node[right] {$Q(u\lam)$} (data);

\node[left=of dummy] (pspace) {$\Lambda$}
   edge[pil,->, bend right=45] node[left] {$u\lam$} (model)
   edge[pil,->, bend left=45] node[above] {$Q\lam$} (data);

\end{tikzpicture}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Measure Theory 101}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{content/problem_formulation.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Philosophical Distinction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we distinguish ourselves from the Bayesian Inverse Problem.}

Bayesian approach: modeling epistemic uncertainties in data on a QoI obtained from a true, but unknown, parameter value, $\paramref$.

\bigskip
\begin{defn}[Deterministic Forward Problem (DFP)]
  Given a space $\pspace$, and QoI map $\qoi$, the \emph{deterministic forward problem} is to determine the values, $\q \in \dspace$ that satisfy
  \begin{equation}
    \q = \qlam, \; \forall \; \param \in \pspace.
  \end{equation}
\end{defn}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we distinguish ourselves from the Bayesian Inverse Problem.}
\begin{defn}[Deterministic Inverse Problem (DIP) Under Uncertainty]
  Given a noisy datum (or data-vector) $d = \q + \xi$, $\q \in \dspace$, the \emph{deterministic inverse problem} is to determine the parameter $\param \in \pspace$ which minimizes
  \begin{equation}
    \norm{\qoi(\param) - d}
  \end{equation}
  where $\xi$ is a random variable (or vector) drawn from a distribution characterizing the uncertainty in observations due to measurement errors.
\end{defn}

In the above definition, $\xi$ is some unobservable perturbation to the true output, arising from epistemic uncertainty (e.g. the precision of available measurement equipment).

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we distinguish ourselves from the Bayesian Inverse Problem.}
The \emph{posterior} is a conditional density, denoted by $\pi_\text{post}(\param\, | \, d)$, proportional to the product of the prior and data-likelihood function \cite{Walpole, Berger, Complete, Smith}:

\begin{equation}\label{eq:sb_post}
    \posterior\lam := \prior\lam \frac{L_\dspace (\q | \param)}{ C },
\end{equation}

where we emphasize the use of $\posterior$ to distinguish the \emph{posterior} from the updated density $\updated$ in \eqref{eq:updated-pdf}.

\bigskip
The \emph{evidence} term $C$ ensures the posterior density integrates to one:
\[
C = \int_\pspace \prior\lam L_\dspace(\q | \param) \, d\param.
\]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we provide an illustrative example.}

\begin{itemize}
\item Suppose $\pspace = [-1,1]\subset\RR$ and $Q(\param)=\param^5$ so that $\dspace = [-1,1]$
\bigskip
\item $\initial\sim \mathcal{U}([-1,1])$ and $\observed\sim N(0.25,0.1^2)$
\bigskip
\item $d\in \dspace$ with $d=Q(\paramref)+\xi$ where $\xi\sim N(0,0.1^2)$
\bigskip
\item $\prior = \initial$ and $d=0.25$ so $L_\dspace = \observed$
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we provide an illustrative example.}

\begin{figure}
\centering
   \includegraphics[width=0.65\linewidth]{figures/bip-vs-sip-1.png}
   \includegraphics[width=0.65\linewidth]{figures/bip-vs-sip-pf-1.png}
 % \caption{(Left) The initial/prior PDF $\initial$ (blue solid curve), updated PDF $\updated$ (black dashed curve), and posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) on $\Lambda$.
 % (Right) The push-forward (PF) of the initial/prior PDF $\predicted$ (blue solid curve), observed/likelihood PDF (red solid curve), PF of the updated PDF $\updated$ (black dashed curve), and the PF of the posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) for the QoI.}
 \label{fig:bayes-comparison}
\end{figure}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we provide an illustrative example.}

\centering
\emph{What happens as we collect more data?}
\bigskip

SIP: Use $N$ to estimate mean of observed \\
DIP: likelihood function incorporates more data

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we provide an illustrative example.}

\begin{figure}
\centering
   \only<1>{
	 \includegraphics[width=0.65\linewidth]{figures/bip-vs-sip-5.png}
   \includegraphics[width=0.65\linewidth]{figures/bip-vs-sip-pf-5.png}
	 }
	 \only<2>{
   \includegraphics[width=0.65\linewidth]{figures/bip-vs-sip-10.png}
   \includegraphics[width=0.65\linewidth]{figures/bip-vs-sip-pf-10.png}
	 }
	 \only<3>{
   \includegraphics[width=0.65\linewidth]{figures/bip-vs-sip-20.png}
   \includegraphics[width=0.65\linewidth]{figures/bip-vs-sip-pf-20.png}
	 }
 \label{fig:bayes-comparison-convergence}
\end{figure}


\end{frame}
