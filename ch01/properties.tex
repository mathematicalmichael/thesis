stableinitial\subsection{Properties and Assumptions of Consistent Update}\label{sec:properties}

To construct a solution $\updatedP$ satisfying \eqref{eq:inverse-problem}, we adopt a Bayesian perspective of combining prior beliefs with data. 

\begin{defn}[Initial Distribution]\label{defn:initial}
  The density $\initial$ is used to represent any prior beliefs about parameters before observations on QoI taken into account, and is referred to as the \emph{initial distribution} (or density).
  It can also be given as a measure $\initialP$, in which case $\initial$ is the Radon-Nikodym derivative of $\initialP$ with respect to the given volume measure $\pmeas$. 
\end{defn}

To construct $\updated$, we ``push-forward'' the prior (initial) beliefs using the QoI map to compare to the evidence provided by $\observed$. 
In other words, we first solve the forward problem of \eqref{defn:forward-problem} to construct a solution to the inverse problem. 
To make this precise, we use the following:

\begin{defn}[Predicted Distribution]\label{defn:predicted}
  The push-forward density of $\initial$ under the map $\qoi$ is denoted as $\predicted$, and is referred to as the \emph{predicted distribution} (or density). 
  It is given as the Radon-Nikodym derivative (with respect to $\dmeas$) of the push-forward probability measure \eqref{eq:forward-problem} given by 
  \begin{equation}\label{eq:predicted}
    \predictedP (E) = P_\pspace \left ( \qoi^{-1}(E) \right ), \; \forall \; E \in \dborel,
  \end{equation}
  which should be recognizable from the definition of the forward problem. 
  In other words, $\predictedP$ is the solution to the push-forward problem given $\initialP$.
\end{defn}


The stochastic inverse problem is defined as finding a measure $\PP_\pspace$ such that the push-forward of it matched $\observedP$.
The following assumption guarantees a solution to the stochastic inverse problem. 
It implies that any event which is assigned a positive probability by the observations must also be assigned a positive probability of occurring by the initial beliefs. 

\begin{assumption}[Predictability Assumption]\label{as:pred}
  The measure associated with $\observed$ is absolutely continuous with respect to the measure associated with $\observed$.
\end{assumption}

If this is unsatisfied, one source of information (the data) suggests certain events are probable while another source of information (the model and initial) have a priori ruled that almost surely these events should not occur. 
Therefore, either initial beliefs, the model under consideration, or the description of uncertainty encoded in $\predicted$ should be subjected to a critical reevaluation. 


The requirement given in Assumption~\ref{as:pred} is guaranteed if the following is satisfied:
\begin{equation}\label{eq:pred}
  \exists \; C>0 \text{ such that } \observed (d) \leq C \predicted(d) \text{ for a.e. } d\in \dspace,
\end{equation}
where it is understood that $d = \qlam$ for some $\param \in \pspace$.
Now, assuming \eqref{eq:pred} holds, we restate the following theorem from \cite{BJW18} based upon the disintegration of measures:


\begin{thm}[Existence and Uniqueness]
  For any set $A\in \pborel$, the solution $\updatedP$ given defined by
  \begin{equation}\label{eq:dci_sol}
    \updatedP (A) = \int_\dspace \left (  \int_{\pspace \in \qoi^{-1}(d)}  \initial\lam \frac{\observed\q}{\predicted\q} \, d\mu_{\pspace, d} \lam \right ) \, d\dmeas(d), \; \forall \; A \in \pborel
  \end{equation} 
  is a consistent solution to the stochastic inverse problem given in (\ref{eq:inverse-problem}), and is uniquely defined up to the specification of a prior (initial) probability measure $P_\pspace$ on $(\pspace, \pborel)$.
  Here, $\mu_{\pspace, d}$ denotes the disintegration of the dominating measure $\mu_\pspace$.
\end{thm}

The updated density \eqref{eq:update} in the iterated integral in \eqref{eq:dci_sol} has no normalization constant because it is in fact a density (i.e., it integrates to $1$), which is summarized in Corollary 3.1 in \cite{BJW18} and restated in simplified form below:
\begin{cor}\label{cor:int}
$\updatedP(\pspace) = 1$.
\end{cor}

These definitions are combined to form the \emph{updated density}, originally derived in \cite{BJW18}:

\begin{defn}[Updated Distribution]\label{defn:updated}
  A solution satisfying \eqref{eq:dci_sol} is referred to as an updated distribution, with an updated density given by the Radon-Nikodym derivative, 
  \begin{equation}\label{eq:update}
    \updated \lam = \initial \lam \frac{\observed \q }{\predicted \q }, \; \forall \; \param \in \pspace.
  \end{equation}
\end{defn}

Corollary~\ref{cor:int} is critical to understanding some significant differences between the classical/statistical Bayesian updated density~\cite{Smith} and the updated density density given by \eqref{eq:update}, which we discuss in Section~\ref{sec:othermethods}.
Moreover, this Corollary provides the basis for a useful numerical diagnostic that informs us about the quality of a solution based on finite sampling and density estimatation.

%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Stability of the Consistent Solution}\label{sec:stability}

We first recall the Total Variation (TV) metric, defined as 
\begin{equation}\label{eq:tv}
d_{\text{TV}} (P_f, P_g) := \int \abs{f - g} \, d\mu,
\end{equation}
where $f,g$ are the densities (Radon-Nikodym derivatives with respect to $\mu$) associated with measures $P_f, P_g$, respectively.
The stability results below are all with respect to the TV metric, which is widely used in the literature and is also known as \emph{statistical distance}~\cite{GS02, Smith, Silverman}.
We first define stability with respect to perturbations in the data.

\begin{defn}[Stability of Updated Densities I]\label{defn:stableobs}
  Given $\initialP$ and $\observedP$, let $\widehat{\observedP}$ be any perturbation to $\observedP$ on $(\dspace, \dborel)$ satisfying \eqref{eq:pred}. 
  Let $\updatedP$ and $\widehat{\updatedP}$ denote the consistent solutions associated with $\observedP$ and $\widehat{\observedP}$, respectively. 
  We say that $\updatedP$ is \emph{stable} with respect to perturbations in $\observedP$ if for all $\eps > 0$, there exists a $\delta > 0$ such that
  \begin{equation}
    d_{\text{TV}} (\observedP, \widehat{\observedP}) < \delta \implies d_{\text{TV}} (\updatedP, \widehat{\updatedP}) < \eps.
  \end{equation}
\end{defn}

In \cite{BJW18}, it is shown that $d_{\text{TV}} (\widehat{\updatedP}, \updatedP) = d_{\text{TV}} (\widehat{\observedP}, \observedP)$, which immediately proves the following:

\begin{thm}
  $\updatedP$ is stable with respect to perturbations to $\observedP$.
  \label{thm:stableobs}
\end{thm}

This next definition and result are useful in analyzing the sensitivity of the updated density with respect to the initial beliefs.

\begin{defn}[Stability of Updated Densities II]\label{defn:stableinitial}
  Given $\initialP$ and $\observedP$, let $\widehat{\initialP}$ be any perturbation to $\initialP$ on $(\pspace, \pborel)$ satisfying \eqref{eq:pred}. 
  Let $\updatedP$ and $\widehat{\updatedP}$ denote the consistent solutions associated with $\observedP$ and $\widehat{\observedP}$, respectively. 
  Let $\sett{\PP_{\pspace, d}}{d\in\dspace}{}$ and $\sett{\widehat{\PP_{\pspace, d}}}{d\in\dspace}{}$ be the conditional probabilities defined by the disintegration of $\initialP$ and $\widehat{\initialP}$, respectively. 
  We say that $\updatedP$ is \emph{stable} with respect to perturbations in $\initialP$ if for all $\eps > 0$, there exists a $\delta > 0$ such that for almost every $d\in\supp(\observedP)$, 
  \begin{equation}\label{eq:stableinitial}
    d_{\text{TV}} (\PP_{\pspace, d}, \widehat{\PP_{\pspace, d}}) < \delta \implies d_{\text{TV}} (\updatedP, \widehat{\updatedP}) < \eps.
  \end{equation}
\end{defn}

\begin{thm}
  $\updatedP$ is stable with respect to perturbations to $\initialP$
  \label{thm:stableinitial}
\end{thm}

Taken together, these stability results provide assurances that the updated density we obtain is ``accurate'' up to the level of experimental error polluting $\observedP$ and error in incorrectly specifying prior assumptions using $\initialP$. 
Given that specifying the definition of a ``true'' initial density is somewhat nebulous, we are less interested in the consequences of the latter conclusion.
However, generating samples from $\updatedP$ requires a numerical approximation to $\predictedP$, which introduces additional errors in $\updatedP$.
If $\widehat{\predicted}$ denotes a computational approximation to the push-forward of the initial density obtained with $\widehat{\predicted}$ substituted for $\predicted$ in \eqref{eq:dci_sol}, then the conditional densities from the Disintegration Theorem are given as
\[
\frac{\widehat{d\PP_{\pspace, d}}}{d\mu_{\pspace, d}\lam} = \frac{\initial\lam}{ \widehat{\predicted\q} },
\]
where $\widehat{\PP_{\pspace, d}}$ denotes the disintegration of $\widehat{\updatedP}$.
In Section~\ref{sec:approx}, the TV metric is used to bound the error in the updated density in terms of the error in the approximation to the push-forward of the initial.



%%%%%%%%% Section 2.3
\subsection{Numerical Approximation and Sampling}\label{sec:approx}
%Since we are given $\initial$ and $\observed$, the computation of $\predicted$ is the only aspect of the Consistent Bayesian framework that needs to be approximated. 
%Since there are few restrictions on the structure of the map $\qoi$ that defines $\predicted$, there is in general no explicit expression from which we can generate samples, so we use a numerical approximation to the probability density function. 
%
%For simplicity, we simply propagate Monte-Carlo samples from the prior and use a kernel density estimate (usually Gaussian\footnote{In this proposal, all results are generated using this kernel, though six kernels common to density estimation are implemented in the ConsistentBayes Python package [TK - cite Silverman and your github].}).
%
%We summarize this in the following algorithm:
%
%\begin{algorithm}[hbtp]
%\DontPrintSemicolon
%Generate a set of samples $\sett{\param_i}{i=1}{N}$
%	\For{$i = 1, \hdots, N$}{
%			Propagate sample $\param_i$ through the QoI map. Set $d_i = \qoi(\param_i)$.
%	}
%Use $\sett{d_i}{i=1}{N}$ and a density estimation method to approximate $\predicted$.
%	\label{alg:sample}
%\caption{Numerical Approximation of the Push-forward of the Prior Density}
%\end{algorithm}
%


%The computational object associated with $\predicted$ is stored for re-use and can be evaluated at locations in $\dspace$ other than $\sett{d_i}{i=1}{N}$. 
%This procedure should be thought of as a characterization of the data space given the prior assumptions encoded in $\initial$.


Using the previous notation, we let $\widehat{\predicted(d)}$ be a computational approximation to $\predicted$ and $\widehat{\updated}$ the associated approximate updated density $\updated$.
As before, we assume the following for the approximation of the push-forward of the initial density:
\begin{assumption}\label{as:predx}
There exists some $C>0$ such that
\[
\observed (d) \leq C \widehat{\predicted(d)} \text{ for a.e. } d\in \dspace.
\]
\end{assumption}

If this assumption is satisfied, we have the following theorem from \cite{BJW18}:
\begin{thm}
  The error in the approximate updated density is bounded above:
  \begin{equation}\label{eq:predicted_bound}
    d_{\text{TV}} (\updatedP, \widehat{\updatedP}) \leq C d_{\text{TV}} (\predictedP, \widehat{\predictedP}),
  \end{equation}
  where the $C$ is the constant taken from Assumption \ref{as:predx}. 
\end{thm}

In practice, we perform a forward propagation of samples from $\pspace$ to $\dspace$ and compute an approximation of $\predicted$ using density estimation~\cite{BJW18}.
Then, we may evaluate $\updated$ directly for any sample of $\pspace$ at the cost of one model solve for any sample other than the ones already propagated to approximate $\predicted$. 
We leverage the re-use of samples in the results herein extensively.

In summary, the \textbf{accuracy of the computed updated density relies on the accuracy of the approximation of the push-forward of the initial.}
Throughout this proposal, we utilize a Gaussian (KDE), which converges at a rate of $\mathcal{O}(N^{-4/(4+d)})$ in mean-squared error and $\mathcal{O}(N^{-2/(4+d)})$ in $L^1$-error, where $d$ is the dimension of $\dspace$, and $N$ is the number of samples from $\initial$ propagated through $\qoi$ [TK - references for convergence rates].

%For simplicity, we introduce the following notation to capture the role of the ratio involved in \eqref{eq:dci_sol} to demonstrate properties we can leverage for generating samples from $\updated$.
%We let
%\[
%\updated\lam = \initial \lam r\q, \text{ where } r\q = \frac{\observed\q}{\predicted\q}
%\]
%
%Many standard calculations about the updated density involve integrals of functions of $r\q$ with respect to the prior.
%For any measurable function $f$, we establish the connection of calculating quantities over $\pspace$ with those over $\dspace$ by leveraging the following identity from measure theory:
%\[
%\int_\pspace f\left( r\q \right ) \, d\initialP = \int_\dspace f\left( r(d) \right ) \, d\predictedP
%\]
%
%We use several throughout this proposal, including the integral of the updated density:
%\[
%I(\updated ) = \int_\pspace r\q \, d\initialP = \int_\dspace r(d) \, d\predictedP ,
%\]
%which we can use to validate that $I(\updated) \approx 1$ in order to numerically validate that the assumption given in \eqref{eq:predicted_bound} was not violated. 
%
%Similarly, we follow \cite{BJW18} to write the commonly used metric for Information Gain, the Kullback-Liebler (KL) divergence:
%\begin{equation}\label{eq:KLdiv}
%\text{KL}(\initial : \updated ) = \int_\pspace r\q \log r\q \, d\initialP = \text{KL}(\observed : \predicted ),
%\end{equation}
%i.e., the KL-divergence between the prior and updated density is equal to the KL-divergence between the observed density and the push-forward of the prior.


Where possible, we express $\predicted$ analytically for purposes of demonstration (especially for comparing numerical solutions to the exact solutions). 