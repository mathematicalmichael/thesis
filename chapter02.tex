\chapter{\uppercase{Background on Data-Consistent Inversion} \label{chapter:02}}

\section{Notation, Terminology, and Assumptions}
We begin by assuming that a (deterministic) model, denoted by $$\M (u, \param) = 0,$$ is specified to relate observable state variables $u$ to model inputs ({\em parameters}) denoted by the vector $\param\in\RP$.
The components $\param_\iparam$ may include parameters in either the model operator (e.g. a diffusion coefficient) or input data (e.g. the frequency of a sinusoidal source, initial, or boundary information).
We allow $\pspace$ to denote the set of all possible input parameters.
We assume $\pspace\subset\RP$ is equipped a (volume) measure, $\pmeas$ on the Borel $\sa$ $\pborel$, defining the measure space $\Pspace$.
The solution operator of the model $\M$ then defines a map taking $\param\in\pspace$ to a solution denoted $u\lam$ which is assumed to be unique.

However, in real experimental settings we are often unable to observe $u\lam$, instead having access to some finite set of observable scalar quantities.
For example, in experiments involving the diffusion of heat, we can typically only record the temperature at some small number of pre-specified points in space-time where measurement devices can be positioned.

Such observable values of $u\lam$ are mathematically modeled by functionals of the solution, denoted $\qoi_\idata: u\lam \to \RR$.
The collection of such functionals into a vector defines a {\em Quantity of Interest} (QoI) map.
Since the solution to the model depends on $\param$, so do the QoI, which motivates the notation
$$\qlam := \qoi( u\lam ) \in\RD,$$
to make this dependence on model parameters explicit.
Furthermore, this convention captures a realistic limitation of an experimental setting, where we may be able to control $\param$ in order to observe $\qlam$, but lack the ability to observe $u\lam$ directly.
The outputs of the QoI map $\qlam = \data$ are what we refer to as the \emph{data}.
Similarly, the range of the QoI defines the \emph{data space} $\dspace$, i.e.
$$\dspace = \qoi(\pspace) \subset \RD$$

We let $\qspace$ denote the set of possible QoI maps for which it is possible to collect experimental data.
For example, suppose we may record only a single temperature measurement at any of ten locations in space-time.
Then $\qspace$ is defined by ten possible QoI maps.
If we can record any two such measurements, then $\qspace$ is defined by $\binom{10}{2} = 45$ possible maps.
Observe that $\qspace$ could easily be uncountable, for example if we were not limited to the spatial locations (or time) at which we could record temperature measurements.
However, for simplicity, we will only discuss problems where $\qspace$ is finite.
In the event that we need to compare maps, we adopt the notation $\dspace_{\qoi}$ to emphasize that the data space depends on the choice of QoI map $\qoi$; when the context is clear, we drop the subscript.
The only assumption on $\qoi$ that we impose throughout this work is that of piecewise-differentiability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \pagebreak
\section{Set-Based Inversion for Measures}\label{sec:ch02-set}
\input{ch02/set_derivation}
\input{ch02/set_error}
\input{ch02/identity_set}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Sample-Based Inversion for Measures}\label{sec:ch02-sample}
[TK - Words]

\begin{equation}
\updatedP = \initialP \frac{\observedP}{\predictedP}
\end{equation}

\begin{equation}
\begin{split}
\dci\\
\dciP\\
\dciD
\end{split}
\end{equation}

In place of the ansatz, we have an initial distribution.

\subsection{Numerical Approximation and Analysis}\label{sec:sample-approx}


\vspace{2in}

\subsection{Descriptions of Error}\label{sec:sample-error}

KDE is now the primary source, show relevant triangle inequality here.
Summarize Troy and Tim's work on the sensitivity analysis of everything.


Then, we have by repeated application of the triangle inequality that
\begin{equation}
\label{eq:sample-triangleineq}
d(\PP_{\pspace, \ndiscs, \nsamps, h}, \paramP) \leq
\underset{ \text{(E1)} }{\underbrace{d(\PP_{\pspace, \ndiscs, \nsamps, h},\PP_{\pspace, \ndiscs, \nsamps})}} +
\underset{ \text{(E2)} }{\underbrace{d(\PP_{\pspace, \ndiscs, \nsamps}, \PP_{\pspace, \ndiscs}) }}+
\underset{ \text{(E3)} }{\underbrace{d(\PP_{\pspace, \ndiscs}, \paramP) }}.
\end{equation}

Talk more about it.

\vspace{2in}

\subsection{Example}\label{sec:sample-example}
Identity Map in two dimensions

\input{ch02/identity_sampling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\
\section{Software Contributions}\label{sec:ch02-software}

\subsection{Background and Motivation}
The open-source software package BET was developed actively from 2012-2015 as part of research performed under grant [TK grant-DOE].
It was originally written in Python 2.7 and is administered by the Computational Hydrology Group at the University of Texas: Austin through their UT-CHG GitHub group [TK - cite Github].
The purpose of this open-source software package was to implement the methods first described in [TK - cite BET papers] for the description and solution of stochastic inverse problems.

In the intermittent years since its original publication in [TK - date of first release, cite Github], the BET package has seen two major releases and the incorporation of several sub-modules (e.g. the functions in {\tt sensitivity} implement much of the original research performed by Dr. Walsh [TK - cite Scott]).


\subsection{Upgrades, Updates, and Features}
Since the last major release [TK - cite latest release], the Python community announced the end of long-term support for Python 2 [TK - cite announcement].
Several of the dependencies in BET have been actively developed in Python 3 with no updates to the Python 2 analogs, which suggested that BET should likely undergo the same transition.

The work summarized in Section~\ref{sec:ch02-sample} was implemented in Python 3 independently by the author through the release of the ConsistentBayes package.
Since that code was used for many of the results that constituted the preliminary results for this work, it made very litle sense to re-implement them in Python 2 for BET given the recent trends in community development.
With funding made available through NSF [TK - cite grant], the opportunity to upgrade BET to Python 3 was the most sensible choice.


\subsubsection{Version 2.1.0}
The upgrade to Python 3.4+ began in January 2019 as a first step to incorporate the new sample-based method into BET.
It was completed in late February.
Major (minor? version? TK) release 2.1.0 [TK - put in release] was designed to provide backwards-compatibility with the Python 2.7 version.
Future installations (starting in 2020) will not limit the versions of some core dependencies in order to accomodate backwards-compatibility with Python 2 (e.g. {\tt numpy}, {\tt scipy}), since this would likely downgrade previously installed software for end-users.


\subsubsection{Version 2.2.0}
Several releases of BET (after the upgrade to Python 3 in v2.1.0), incorporated developments that will be discussed herein.
For


\subsubsection{Version 2.2.1}
Major bugfix for parallel testing allowed tests to pass for more than 2 processors.
For some tests, this involved changing the setup parameters to ensure the problem was large enough to break up onto up to 8 processors.
For others, siginficant changes had to be made to structure to allow for proper saving and loading of files in parallel.


\subsubsection{Version 2.3.0}
This release incorporated the sampling-based approach first dicussed in Section~\ref{sec:ch02-sec:ch02-sample}.


\subsection{Examples in BET}
Basic plotting functionality of BET is demonstrated in iPython notebooks [TK - some kind of citation here], which have seen an exponential growth rate on GitHub, and can be edited by the end-user to work with different plotting library versions and backends.
These notebooks were originally created to reflect the example suite in BET (which were {\tt .py} files), but later they were migrated into a separate repository BET-examples to allow for better organization.
In the new framework, each notebook functions as an independent example.
Several of these notebooks were adapted from the example code in this thesis repository.


\section{Illustrative Examples}\label{sec:ch02-examples}
In some examples, we do not work with any model $\M$ and make observations $\qoi$ directly on the data space $\dspace$, so term (E1) in Eq.~\eqref{eq:set-triangleineq} is identically zero for the examples we present in Section \ref{sec:ch02-examples}.
Furthermore, since the probabilities we introduce on $\dspace$ in the numerical results are uniform and our maps linear, the densities can be described analytically with a characteristic function.
In this event, the solution $\paramP$ to the SIP can be given exactly by a change of variables formula, so the inverse set can be known exactly.
When we invert characteristic functions, our solutions will also be members of this same family if the choice of \emph{ansatz} (or \emph{initial density}) is taken to be uniform over $\pspace$.
These examples allow us to study the DCI method for a class of functions for which a solution is readily available, and serves as a requisite testing ground before advancing to more nuanced problem definitions.
We present a brief overview of the factors that influence our practical ability to accurately approximate $\paramP$ and $\updated$ using finite sampling.

For the set-based approach discussed in \ref{sec:ch02-set}, it is desirable that $\ndiscs$ is chosen without respect to $\nsamps$ so that (E3) = $d(\PP_{\pspace, \ndiscs}, \paramP)$ from \ref{sec:set-error} has been made sufficiently small or eliminated entirely.
This amounts to saying that the decision about how to discretize the uncertainty in $\dspace$ is made a priori to cater to some problem specifications.
We choose to impose uniform distributions on $\dspace$ so that the set-valued analog to $\observed$ is perfectly described with $\ndiscs=1$.
Therefore, we focus our attention on the source of error introduced by $d(\PP_{\pspace, \ndiscs, \nsamps}, \PP_{\pspace, \ndiscs} )$, the primary contribution of error in Eq.~\eqref{eq:set-triangleineq}, which is given by the term (E2) = $d(\PP_{\pspace, \ndiscs, \nsamps}, \PP_{\pspace, \ndiscs})$.

Since there is no error introduced from discretizing $\pspace$ in the sample-based approach from \ref{sec:ch02-sample}, the term (E2) is not what contributes to error in the sample-based approach.
As discussed in \ref{sec:sample-error}, the impact of $\nsamps$ is on our ability to characterize $\dspace$.
In situations where an analytical $\predicted$ is not known, we must rely on some form of density estimation.

\input{ch02/decay_set_vs_sample}

\input{ch02/heat_1drod_set_vs_sample}
