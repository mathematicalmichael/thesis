%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation and Terminology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we describe why any of this matters.}

{\bf Broad Goals of Uncertainty Quantification}

\medskip
    \begin{itemize}
	    \item Make inferences and predictions

			\bigskip
        \item Quantify and reduce uncertainties (aleotoric, epistemic)

			\bigskip
	    \item Be \emph{accurate} and \emph{precise}

			\bigskip
	    \item Design ``efficient'' experiments

			\bigskip
	    \item Collect and use data ``intelligently''
    \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{\it The one where we define the letters we use and what they mean.}
\begin{itemize}
	\item State variable: $u$ {\color{gray}(e.g. heat, energy, pressure, deflection)}

	\bigskip
	\item<.-> Parameters: $\lambda$ {\color{gray}(e.g. source term, diffusion, boundary data)}

	\bigskip
		\item Model: $\mathcal{M} (u, \lambda) = 0$, so $u(\param)$

	\bigskip
	\item Quantity of Interest \tdeepred{(QoI)} map, (piecewise smooth):

		$$Q = \mat{c}{q_1\\ q_2\\ \vdots \\ q_\dimD}, \; \text{ where } \; q_i: u(\lambda) \to \RR$$

	\bigskip
	\item We write $Q(\lambda) := Q(u(\lambda))$ to make the dependence on $\param$ explicit.
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we illustrate how a QoI map relates inputs to outputs.}
%\vskip 25pt
\centering
\begin{figure}
\centering

\begin{tikzpicture}[node distance=2cm, auto,]
 %nodes

% dummy figure to make everything look nice
\node[] (dummy) {};

\node[right=of dummy] (data) {$\dspace = \qoi(\pspace)$};

\node[punkt, inner sep=5pt, below=of dummy] (model) {Model \\ (e.g., PDE)}
 	edge[pil,->, bend right=25] node[right] {$Q(u\lam)$} (data);

\node[left=of dummy] (pspace) {$\Lambda$}
   edge[pil,->, bend right=45] node[left] {$u\lam$} (model)
   edge[pil,->, bend left=45] node[above] {$Q\lam$} (data);

\end{tikzpicture}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Measure Theory 101}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{content/problem_formulation.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Philosophical Distinction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]

{\bf Bayesian approach:}

\medskip
\begin{itemize}
	\item Modeling epistemic uncertainties in data.

	\bigskip
	\item Data obtained from a true, but unknown, parameter value, $\paramref$.

	\bigskip
	\item Fundamentally solving a different problem.
\end{itemize}

\bigskip
\begin{defn}[Deterministic Forward Problem (DFP)]
  Given a space $\pspace$, and QoI map $\qoi$, the \emph{deterministic forward problem} is to determine the values, $\q \in \dspace$ that satisfy
  \begin{equation}
    \q = \qlam, \; \forall \; \param \in \pspace.
  \end{equation}
\end{defn}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{defn}[Deterministic Inverse Problem (DIP) Under Uncertainty]
  Given a noisy datum (or data-vector) $d = \q + \xi$, $\q \in \dspace$, the \emph{deterministic inverse problem} is to determine the parameter $\param \in \pspace$ which minimizes
  \begin{equation}
    \norm{\qoi(\param) - d}
  \end{equation}
  where $\xi$ is a random variable (or vector) drawn from a distribution characterizing the uncertainty in observations due to measurement errors.
\end{defn}

\begin{itemize}
	\item $\xi$ is some unobservable perturbation to the true output.
	\item $\xi$ arises from epistemic uncertainty (e.g. the precision of available measurement equipment).
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we distinguish ourselves from the Bayesian Inverse Problem.}

\begin{itemize}
	\item The \emph{posterior} is a conditional density: $$\posterior(\param\, | \, d)$$

	\bigskip
	\item $\posterior$ proportional to the product of $\prior$ and $L_\dspace$ \cite{Walpole, Berger, Complete, Smith}:

\begin{equation}\label{eq:sb_post}
    \posterior(\param\, | \, d) := \prior\lam \frac{L_\dspace (\q | \param)}{ C }
\end{equation}

	\bigskip
	\item The \emph{evidence} term $C$ ensures integration to unity. Given by:

$$C = \int_\pspace \prior\lam L_\dspace(\q | \param) \, d\param$$

\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we provide an illustrative example.}

\begin{itemize}
\item Suppose $\pspace = [-1,1] \subset \RR$ and $Q(\param)=\param^5$ so that $\dspace = [-1,1]$

\bigskip
\item $\initial \sim \mathcal{U}([-1,1])$

\bigskip
\item $\observed \sim N(0.25,0.1^2)$

\bigskip
\bigskip
\item $d\in \dspace$ with $d=Q(\paramref)+\xi$ where $\xi\sim N(0,0.1^2)$

\bigskip
\item $\prior = \initial$ and $d=0.25$ so $L_\dspace = \observed$
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we provide an illustrative example.}

\begin{figure}
\centering
\vbox{
   \includegraphics[width=0.6\linewidth]{figures/bip-vs-sip-1.png}
   \includegraphics[width=0.6\linewidth]{figures/bip-vs-sip-pf-1.png}
}
 % \caption{(Left) The initial/prior PDF $\initial$ (blue solid curve), updated PDF $\updated$ (black dashed curve), and posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) on $\Lambda$.
 % (Right) The push-forward (PF) of the initial/prior PDF $\predicted$ (blue solid curve), observed/likelihood PDF (red solid curve), PF of the updated PDF $\updated$ (black dashed curve), and the PF of the posterior PDF $\pi_\text{post}$ (green dashed-dotted curve) for the QoI.}
 \label{fig:bayes-comparison}
\end{figure}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we provide an illustrative example.}

\centering
\emph{What happens as we collect more data?}
\bigskip

\bigskip
SIP: Use $N$ to estimate mean of observed.

\bigskip
DIP: Likelihood function incorporates more terms.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\it The one where we provide an illustrative example.}

\begin{figure}
\centering
   \only<1>{\vbox{
	 \includegraphics[width=0.55\linewidth]{figures/bip-vs-sip-5.png}
   \includegraphics[width=0.55\linewidth]{figures/bip-vs-sip-pf-5.png}
	 }}
	 \only<2>{\vbox{
   \includegraphics[width=0.55\linewidth]{figures/bip-vs-sip-10.png}
   \includegraphics[width=0.55\linewidth]{figures/bip-vs-sip-pf-10.png}
	 }}
	 \only<3>{\vbox{
   \includegraphics[width=0.55\linewidth]{figures/bip-vs-sip-20.png}
   \includegraphics[width=0.55\linewidth]{figures/bip-vs-sip-pf-20.png}
	 }}
 \label{fig:bayes-comparison-convergence}
\end{figure}


\end{frame}
